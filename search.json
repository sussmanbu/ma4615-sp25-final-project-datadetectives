[
  {
    "objectID": "data.html#wherehow-to-find-data",
    "href": "data.html#wherehow-to-find-data",
    "title": "Data",
    "section": "Where/How to find Data",
    "text": "Where/How to find Data\nThe original dataset is the NLS National Longitudinal Survey, which can be found here. This data has been collected by the U.S. Bureau of Labor Statistics from 1970 - 2020 as part of a longitudinal study to better understand trends of labor force participation, education, and employment experiences evolving over time (with considerations of the effects of certain miscellaneous factors like race, gender, and socioeconomic status). This information will help paint a picture of the impacts the 2008 recession had on different racial and wealth groups in the United States of America.\nTo further explore recession disparities, the NLS dataset was to be combined with the interest rates archive of the Department of Treasury, which can be found here. The data is presented daily, can join average interest rate each year with yearly available data in our existing NLSY data, mainly income. This will highlight any relationships between annual interest rates and income stratified by race. All-in-all, analyzing this datset, in combination with the NLS dataset, will provide a clear picture of macroeconomic trends, and how they effect different race groups in America.\n\nDataSet #1 - NLS National Longitudinal Survey\n\nAll the R scripts we used to load and clean this dataset can be found here. Our cleaned dataset can be found here.\n\n\n\n\nNLS Variable Descriptions\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nCase_ID\nUnique identifier for each case or respondent\n\n\nSample_ID\nID representing the respondent’s sample group\n\n\nIncome_2000\nReported personal income in the year 2000 (in USD)\n\n\nIncome_2002\nReported personal income in the year 2002 (in USD)\n\n\nIncome_2004\nReported personal income in the year 2004 (in USD)\n\n\nIncome_2006\nReported personal income in the year 2006 (in USD)\n\n\nIncome_2008\nReported personal income in the year 2008 (in USD)\n\n\nIncome_2010\nReported personal income in the year 2010 (in USD)\n\n\nAge_2010\nRespondent’s age as of the year 2010\n\n\nRace\nSelf-reported race/ethnicity of the respondent\n\n\nSex\nSex of the respondent (e.g., Male/Female)\n\n\nHighest_Grade_Completed\nHighest level of education or grade completed by 2010\n\n\nMarital_Status\nMarital status of the respondent in 2010\n\n\nRegion\nGeographical region of residence in 2010"
  },
  {
    "objectID": "data.html#data-cleaning---nls-national-longitudinal-survey",
    "href": "data.html#data-cleaning---nls-national-longitudinal-survey",
    "title": "Data",
    "section": "Data Cleaning - NLS National Longitudinal Survey",
    "text": "Data Cleaning - NLS National Longitudinal Survey\nOur data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove “non-interview” data points that we can’t compare across years, and decode integer values representing race, grade, etc. For this process, we primarily depended on the tidyverse library and other methods discussed in lecture.\nnls_data &lt;- nls_data %&gt;%\n  mutate(\n    R6909701 = na_if(R6909701, -1),\n    R6909701 = ifelse(R6909701 &lt; 0, NA, R6909701),\n    R7607800 = na_if(R7607800, -1),\n    R7607800 = ifelse(R7607800 &lt; 0, NA, R7607800),\n    R8316300 = na_if(R8316300, -1),\n    R8316300 = ifelse(R8316300 &lt; 0, NA, R8316300),\n    T0912400 = na_if(T0912400, -1),\n    T0912400 = ifelse(T0912400 &lt; 0, NA, T0912400),\n    T2076700 = na_if(T2076700, -1),\n    T2076700 = ifelse(T2076700 &lt; 0, NA, T2076700),\n    T3045300 = na_if(T3045300, -1),\n    T3045300 = ifelse(T3045300 &lt; 0, NA, T3045300)\n  ) %&gt;%\nOur next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values. In doing so, we discovered other null values that required further debugging and cleaning.\n  # Remove rows where any variable is -5\n  filter(if_all(everything(), ~ . != -5))\nTo begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes.\n# 3. Define Grade Labels\ngrade_labels &lt;- c(\n  \"1st Grade\", \"2nd Grade\", \"3rd Grade\", \"4th Grade\", \"5th Grade\", \n  \"6th Grade\", \"7th Grade\", \"8th Grade\", \"9th Grade\", \"10th Grade\", \n  \"11th Grade\", \"12th Grade\", \"1st Year College\", \"2nd Year College\", \n  \"3rd Year College\", \"4th Year College\", \"5th Year College\", \"6th Year College\", \n  \"7th Year College\", \"8th Year College or More\", \"UNGRADED\"\n)\nAnother challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = “black”, 2 = “white”, etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns.\n# 4. Create Cleaned Dataset\nnls_data_clean &lt;- nls_data %&gt;%\n  mutate(\n    Race = case_when(\n      R0214700 == 1 ~ \"Hispanic\",\n      R0214700 == 2 ~ \"Black\",\n      R0214700 == 3 ~ \"Non-Black/Non-Hispanic\",\n      TRUE ~ NA_character_\n    ),\n    Sex = case_when(\n      R0214800 == 1 ~ \"Male\",\n      R0214800 == 2 ~ \"Female\",\n      TRUE ~ NA_character_\n    ),\n      Highest_Grade_Completed = case_when(\n        T2272800 == 1 ~ \"1ST GRADE\", \n        T2272800 == 2 ~ \"2ND GRADE\", \n        T2272800 == 3 ~ \"3RD GRADE\",\n        T2272800 == 4 ~ \"4TH GRADE\", \n        T2272800 == 5 ~ \"5TH GRADE\", \n        T2272800 == 6 ~ \"6TH GRADE\", \n        T2272800 == 7 ~ \"7TH GRADE\", \n        T2272800 == 8 ~ \"8TH GRADE\", \n        T2272800 == 9 ~ \"9TH GRADE\", \n        T2272800 == 10 ~ \"10TH GRADE\", \n        T2272800 == 11 ~ \"11TH GRADE\", \n        T2272800 == 12 ~ \"12TH GRADE\", \n        T2272800 == 13 ~ \"1ST YEAR COLLEGE\", \n        T2272800 == 14 ~ \"2ND YEAR COLLEGE\", \n        T2272800 == 15 ~ \"3RD YEAR COLLEGE\", \n        T2272800 == 16 ~ \"4TH YEAR COLLEGE\", \n        T2272800 == 17 ~ \"5TH YEAR COLLEGE\", \n        T2272800 == 18 ~ \"6TH YEAR COLLEGE\", \n        T2272800 == 19 ~ \"7TH YEAR COLLEGE\", \n        T2272800 == 20 ~ \"8TH YEAR COLLEGE OR MORE\", \n        T2272800 == 95 ~ \"UNGRADED\",\n        TRUE ~ NA_character_\n      ),\n  Marital_Status = case_when(\n      T3108400 == 0 ~ \"Never Married\",\n      T3108400 == 1 ~ \"Married\",\n      T3108400 == 2 ~ \"Separated\",\n      T3108400 == 3 ~ \"Divorced\",\n      T3108400 == 6 ~ \"Widowed\",\n      TRUE ~ NA_character_\n    ),\n  Region = case_when(\n    T3108200 == 1 ~ \"Northeast\",\n    T3108200 == 2 ~ \"North Central\",\n    T3108200 == 3 ~ \"South\",\n    T3108200 == 4 ~ \"West\",\n    TRUE ~ NA_character_\n  )\n  ) %&gt;%\n  # Convert categorical variables to factors\n  mutate(across(c(Race, Sex, Highest_Grade_Completed, Marital_Status, Region), as.factor)) %&gt;%\n  # Remove original numeric columns\n  select(-c(R0214700, R0214800, T2272800, T3108400, T3108200)) %&gt;%\n  # Rename columns\n  # Rename columns\n  rename(\n    Case_ID = R0000100,\n    Sample_ID = R0173600,\n    Age_2010 = T3108700,\n    Income_2000 = R6909701,\n    Income_2002 = R7607800,\n    Income_2004 = R8316300,\n    Income_2006 = T0912400,\n    Income_2008 = T2076700,\n    Income_2010 = T3045300\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNULL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataSet #2 - Interest Rates by Year\n\nThis cleaned dataset can be found here.\n\n\n\n\nInterest Rate Variable Descriptions\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nYear\nYear the data was collected\n\n\nmean_LT\nMean of long-term interest rates (&gt;10 years)\n\n\nmin_LT\nMinimum of long-term interest rates (&gt;10 years)\n\n\nmax_LT\nMaximum of long-term interest rates (&gt;10 years)\n\n\nsd_LT\nStandard deviation of long-term interest rates (&gt;10 years)\n\n\nmedian_LT\nMedian of long-term interest rates (&gt;10 years)\n\n\nmode_LT\nMode of long-term interest rates (&gt;10 years)\n\n\nmean_TREASURY\nMean of mortgage interest rates\n\n\nmin_TREASURY\nMinimum of mortgage interest rates\n\n\nmax_TREASURY\nMaximum of mortgage interest rates\n\n\nsd_TREASURY\nStandard deviation of mortgage interest rates\n\n\nmedian_TREASURY\nMedian of mortgage interest rates\n\n\nmode_TREASURY\nMode of mortgage interest rates"
  },
  {
    "objectID": "data.html#data-cleaning---interest-rates-by-year",
    "href": "data.html#data-cleaning---interest-rates-by-year",
    "title": "Data",
    "section": "Data Cleaning - Interest Rates by Year",
    "text": "Data Cleaning - Interest Rates by Year\nWe began by importing the dataset using the read_csv() function from the readr package in the tidyverse. Then, we first added additional income years to cover 2000-2010 as opposed to just 2010. Afterwards, we joined the interest rate average of each year with the corresponding two years - so 2001 and 2002 went into 2002, 2003 and 2004 went into 2004, and so on. By doing this, we were able to join the interest rate data into our survey data in an efficient and complementary way.\n# Load in another library to clean second dataset\nlibrary(lubridate)\n\n# Now we clean the second Dataset which we loaded in \nrates &lt;- read_csv(\"./dataset/long-term-rates-2000-2010.csv\")\nrates2 &lt;- read_csv(\"./dataset/long-term-rates-2011-2020.csv\")\n\n#combine rates and rates2\ncombined_rates &lt;- bind_rows(rates1, rates2)\nAdditionally, defining a custom get_mode() function, made it easy to handle mode calculation, since R does not provide a built-in mode() for numeric data in a tidy pipeline.\n# Define a mode function (returns the first mode if multiple)\nget_mode &lt;- function(x) {\n  ux &lt;- unique(na.omit(x))\n  ux[which.max(tabulate(match(x, ux)))]\n}\nThe Date column, originally in character format, was converted to proper Date objects using lubridate::mdy(). We then extracted the year from each date to enable grouping by year. To understand yearly trends in interest rates, we grouped the data by Year and computed a comprehensive set of summary statistics for each of the two rate columns including mean, minimum, maximum, standard deviation, median, and mode.\n#mutate data to have proper year date format (wil be easier to compute stat summaries with)\ninterest_rates_cleaned &lt;- combined_rates %&gt;%\n  mutate(Date = mdy(Date),\n         Year = year(Date)) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    mean_LT = mean(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    min_LT = min(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    max_LT = max(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    sd_LT = sd(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    median_LT = median(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    mode_LT = get_mode(`LT COMPOSITE (&gt;10 Yrs)`),\n    \n    mean_TREASURY = mean(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    min_TREASURY = min(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    max_TREASURY = max(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    sd_TREASURY = sd(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    median_TREASURY = median(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    mode_TREASURY = get_mode(`TREASURY 20-Yr CMT`)\n  ) %&gt;%\n  ungroup()\nFinally, the dataset is written on to a CSV file for later use.\nwrite_csv(interest_rates_cleaned, \"interest_rates_cleaned.csv\")"
  },
  {
    "objectID": "data.html#data-combination",
    "href": "data.html#data-combination",
    "title": "Data",
    "section": "Data Combination",
    "text": "Data Combination\n\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your clean_data.R file is fine and encouraged if it helps explain what you did.\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show long quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "analysis.html#introduction",
    "href": "analysis.html#introduction",
    "title": "Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nObjective and Research Questions\nThe goal of our analysis is to examine how macroeconomic trends in the aftermath of the 2008 recession, such as shifting interest rates, affected different racial and socioeconomic groups in the United States. We aim to answer some of the following research questions:\n\nWhat factors contribute to upward economic mobility?\nHow do income and employment differ across gender, race, and education levels?\nAre there persistent racial disparities in long-term wealth accumulation?\n\n\n\nMotivating Figures and Tables\nThe following graphs explore some of the relationships between variables in our data to motivate this goal.\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\n#define correct order of education levels(needed for next query)\neducation_levels &lt;- c(\n  \"1ST GRADE\", \"2ND GRADE\", \"3RD GRADE\", \"4TH GRADE\", \"5TH GRADE\", \"6TH GRADE\", \n  \"7TH GRADE\", \"8TH GRADE\", \"9TH GRADE\", \"10TH GRADE\", \"11TH GRADE\", \"12TH GRADE\", \n  \"1ST YEAR COLLEGE\", \"2ND YEAR COLLEGE\", \"3RD YEAR COLLEGE\", \"4TH YEAR COLLEGE\", \n  \"5TH YEAR COLLEGE\", \"6TH YEAR COLLEGE\", \"7TH YEAR COLLEGE\", \"8TH YEAR COLLEGE OR MORE\",\n  \"UNGRADED\", NA\n)\n\n#mutate to order based on most education completed \nnls_data_clean &lt;- nls_data_clean %&gt;%\n  mutate(Highest_Grade_Completed = factor(Highest_Grade_Completed, \n                                          levels = education_levels,\n                                          ordered = TRUE))\n#Calcualte average mean_income for each grade category\nincome_by_grade &lt;- nls_data_clean %&gt;%\n  group_by(Highest_Grade_Completed) %&gt;%\n  summarise(mean_income = mean(Income_2010, na.rm = TRUE)) %&gt;%\n  filter(!is.na(Highest_Grade_Completed))\n\n#income vs highest grade plotted with ggplot\nggplot(income_by_grade, aes(x = Highest_Grade_Completed, y = mean_income, group = 1)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(color = \"darkred\", size = 2) +\n  labs(title = \"Average Income by Highest Grade Completed\",\n       x = \"Highest Grade Completed\",\n       y = \"Average Income\") +\n  scale_y_continuous(labels = label_comma()) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\n\nlibrary(scales)\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\nnls_data_clean &lt;- nls_data_clean %&gt;%\n  filter(!is.na(Region))\n\n# Regional Variation in Gender Income Gap\nggplot(nls_data_clean, aes(x = Region, y = Income_2010, fill = Sex)) +\n  geom_boxplot(outlier.alpha = 0.5, outlier.size = 1) +\n  labs(title = \"Income Distribution by Region and Gender\",\n       x = \"Region\", \n       y = \"Total Income (Previous Year)\", \n       fill = \"Gender\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\nThis box plot explores the depth of the relationship between income, gender and region to examine whether there are any disparities between genders.\n\nlibrary(scales)\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\nnls_data_clean_filtered &lt;- nls_data_clean %&gt;%\n  filter(!is.na(Income_2010), is.finite(Income_2010))\n\nggplot(nls_data_clean_filtered, aes(x = factor(Age_2010), y = Income_2010)) + geom_boxplot() + facet_wrap(~ Sex + Race) + scale_y_continuous(labels = dollar_format(prefix = \"$\", suffix = \"K\", scale = 1/1000)) + labs(x = \"Age in 2010\", y = \"Total Income for Year\", title = \"Total Income For Year 2010 Filtered by Age, Race, and Sex\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nNow, we look at interest rates.\n\nlibrary(tidyverse)\n\n# Load the cleaned interest rate data\ninterest_rates &lt;- read_csv(here(\"dataset\", \"interest_rates_cleaned.csv\"))\n\nRows: 11 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (13): Year, mean_LT, min_LT, max_LT, sd_LT, median_LT, mode_LT, mean_TRE...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Map rate years to income years\nrate_to_income &lt;- tribble(\n  ~Year, ~Income_Year,\n  2000, 2000,\n  2001, 2002,\n  2002, 2002,\n  2003, 2004,\n  2004, 2004,\n  2005, 2006,\n  2006, 2006,\n  2007, 2008,\n  2008, 2008,\n  2009, 2010,\n  2010, 2010\n)\n\n# Join and average interest rates by income year\nmapped_rates &lt;- rate_to_income %&gt;%\n  left_join(interest_rates, by = \"Year\") %&gt;%\n  group_by(Income_Year) %&gt;%\n  summarise(avg_LT_rate = mean(mean_LT, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Step 1: Pivot NLS data to long format while keeping demographic columns\nnls_income_long &lt;- nls_data_clean %&gt;%\n  pivot_longer(\n    cols = starts_with(\"Income_\"),\n    names_to = \"Income_Year\",\n    names_prefix = \"Income_\",\n    values_to = \"Income\"\n  ) %&gt;%\n  mutate(Income_Year = as.integer(Income_Year))\n\n# Step 2: Join average interest rates by income year\nnls_with_rates_full &lt;- nls_income_long %&gt;%\n  left_join(mapped_rates, by = \"Income_Year\")\n\nwrite_csv(nls_with_rates_full, here(\"dataset\", \"nls_with_rates_full.csv\"))\n\nincome_by_race &lt;- nls_with_rates_full %&gt;%\n  group_by(Income_Year, Race) %&gt;%\n  summarise(\n    avg_income = mean(Income, na.rm = TRUE),\n    avg_interest = mean(avg_LT_rate, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Income_Year'. You can override using the\n`.groups` argument.\n\nlibrary(ggplot2)\n\nggplot(income_by_race, aes(x = Income_Year, y = avg_income, color = avg_interest)) +\n  geom_line(aes(group = Race), size = 1.2) +\n  geom_point(aes(shape = Race), size = 2) +\n  facet_wrap(~ Race) +\n  scale_color_viridis_c(option = \"magma\") +\n  labs(\n    title = \"Average Income Over Time by Race (Colored by Interest Rate)\",\n    x = \"Year\",\n    y = \"Average Income\",\n    color = \"Interest Rate\",\n    shape = \"Race\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "analysis.html#modeling-and-inference",
    "href": "analysis.html#modeling-and-inference",
    "title": "Analysis",
    "section": "Modeling and Inference",
    "text": "Modeling and Inference\n\n#library(scales)\n# library(car)\n# library(sandwich)\n# library(lmtest)\n# \n# nls_data_tbu &lt;- read_csv(\"dataset/nls_with_rates_full.csv\")\n# \n# nls_data_modeling &lt;- nls_data_tbu %&gt;%\n#   filter(Income_Year %in% c(2006, 2008, 2010))\n# \n# model = lm(Income ~ Race + Sex + Marital_Status + Age_2010 + Region + Highest_Grade_Completed + avg_LT_rate, data = filter(nls_data_modeling, Income &gt; 0))\n# \n# # coeftest(model, vcov = vcovHC(model, type = \"HC1\"))\n# \n# model_data = model.frame(model)\n# model_data$Predicted = predict(model)\n# summary(model)\n# vif(model, type = \"predictor\")\n# # plot(model)\n###"
  },
  {
    "objectID": "analysis.html#results-limitations-and-conclusion",
    "href": "analysis.html#results-limitations-and-conclusion",
    "title": "Analysis",
    "section": "Results, Limitations, and Conclusion",
    "text": "Results, Limitations, and Conclusion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Cost of Crisis: Economic Inequality in the Wake of U.S. Macroeconomic Shocks",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #7\n\n\n\n\n\nThis is Data Detectives’ 7th blog post, where we describe our thoughts and progress on the interactive for our final project \n\n\n\n\n\nApr 21, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #6\n\n\nExploratory Data Analysis\n\n\nThis is data detectives’ sixth blog post, where we continue our work. \n\n\n\n\n\nApr 14, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #5\n\n\nExploratory Data Analysis\n\n\nThis is data detectives’ fifth blog post, where we add a new data set about interest rates. \n\n\n\n\n\nApr 7, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #4\n\n\nExploratory Data Analysis\n\n\nThis is data detectives’ fourth blog post, where we do some initial exploratory data analysis on our cleaned data set. \n\n\n\n\n\nMar 28, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3\n\n\nData Loading/Cleaning\n\n\nThis is Data Detectives’ third blog post where we further investigate, load, and clean our initial dataset. \n\n\n\n\n\nMar 24, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #2\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nPatrio Marcus, Chandini Toleti, Ranbir, Aretha McDonald, Emir\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #1\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nPatrio Marcus, Chandini Toleti, Ranbir, Aretha McDonald, Emir\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-03-28-blog-post-4/blog-post-4.html",
    "href": "posts/2025-03-28-blog-post-4/blog-post-4.html",
    "title": "Blog Post #4",
    "section": "",
    "text": "For this blog post some members of our team each did exploratory analysis into our Data.\n\nChandini’s Graph\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\n#define correct order of education levels(neeeded for next query)\neducation_levels &lt;- c(\n  \"1ST GRADE\", \"2ND GRADE\", \"3RD GRADE\", \"4TH GRADE\", \"5TH GRADE\", \"6TH GRADE\", \n  \"7TH GRADE\", \"8TH GRADE\", \"9TH GRADE\", \"10TH GRADE\", \"11TH GRADE\", \"12TH GRADE\", \n  \"1ST YEAR COLLEGE\", \"2ND YEAR COLLEGE\", \"3RD YEAR COLLEGE\", \"4TH YEAR COLLEGE\", \n  \"5TH YEAR COLLEGE\", \"6TH YEAR COLLEGE\", \"7TH YEAR COLLEGE\", \"8TH YEAR COLLEGE OR MORE\",\n  \"UNGRADED\", NA\n)\n\n#mutate to order based on most education completed \nnls_data_clean &lt;- nls_data_clean %&gt;%\n  mutate(Highest_Grade_Completed = factor(Highest_Grade_Completed, \n                                          levels = education_levels,\n                                          ordered = TRUE))\n#Calcualte average mean_income for each grade category\nincome_by_grade &lt;- nls_data_clean %&gt;%\n  group_by(Highest_Grade_Completed) %&gt;%\n  summarise(mean_income = mean(Total_Income_Prev_Year, na.rm = TRUE)) %&gt;%\n  filter(!is.na(Highest_Grade_Completed))\n\n#income vs highest grade plotted with ggplot\nggplot(income_by_grade, aes(x = Highest_Grade_Completed, y = mean_income, group = 1)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(color = \"darkred\", size = 2) +\n  labs(title = \"Average Income by Highest Grade Completed\",\n       x = \"Highest Grade Completed\",\n       y = \"Average Income\") +\n  scale_y_continuous(labels = label_comma()) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nHere, we analyze the relationship between highest grade completed and average income. In order to create this plot, we had to order each grade group and calculate mean incomes for each one. Finally, we discovered the positive relationship we predicted. This data shows that the more education people completed, the higher their income. However, we also see the presence of outliers impacting the data’s skew. Regardless, these variables will be useful for future data analysis when we combine our data with recession data. Hopefully, we can compare these variables to recession information and compare the relationship between them before and after.\n\n\nAretha’s Graph\n\nlibrary(scales)\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\nnls_data_clean &lt;- nls_data_clean %&gt;%\n  filter(!is.na(Region))\n\n# Regional Variation in Gender Income Gap\nggplot(nls_data_clean, aes(x = Region, y = Total_Income_Prev_Year, fill = Sex)) +\n  geom_boxplot(outlier.alpha = 0.5, outlier.size = 1) +\n  labs(title = \"Income Distribution by Region and Gender\",\n       x = \"Region\", \n       y = \"Total Income (Previous Year)\", \n       fill = \"Gender\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 359 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThis box plot explores the depth of the relationship between income, gender and region to examine whether there are any disparities between genders. In order to plot this graph, I had to add the ‘region’ column which describes which region of the US participants reside in - this usually has an impact on base salaries and living expenses. We believe this data column will become increasingly useful in further analysis as we look to add recession data as our 2nd dataset. We can potentially look how certain regions were impacted (based on income/debt) in comparison to others.\n\n\nRan’s Model\n\nlibrary(scales)\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\nnls_data_clean_filtered &lt;- nls_data_clean %&gt;%\n  filter(!is.na(Total_Income_Prev_Year), is.finite(Total_Income_Prev_Year))\n\nset.seed(1234)\nmodel = lm(Total_Income_Prev_Year ~ Race + Sex + Marital_Status, data = nls_data_clean_filtered)\nmodel_data = model.frame(model)\nmodel_data$Predicted = predict(model)\nsummary(model)\n\n\nCall:\nlm(formula = Total_Income_Prev_Year ~ Race + Sex + Marital_Status, \n    data = nls_data_clean_filtered)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-63483 -28742  -9317  14905 295913 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    16411       1617  10.151  &lt; 2e-16 ***\nRaceHispanic                    3986       1670   2.387  0.01701 *  \nRaceNon-Black/Non-Hispanic     14684       1369  10.725  &lt; 2e-16 ***\nSexMale                        19741       1140  17.315  &lt; 2e-16 ***\nMarital_StatusMarried          12647       1470   8.605  &lt; 2e-16 ***\nMarital_StatusNever Married    -5821       1899  -3.066  0.00218 ** \nMarital_StatusSeparated        -8236       2808  -2.933  0.00337 ** \nMarital_StatusWidowed          -8836       4206  -2.101  0.03566 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48050 on 7187 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.09739,   Adjusted R-squared:  0.09651 \nF-statistic: 110.8 on 7 and 7187 DF,  p-value: &lt; 2.2e-16\n\nvif(model)\n\n                   GVIF Df GVIF^(1/(2*Df))\nRace           1.108749  2        1.026144\nSex            1.011891  1        1.005928\nMarital_Status 1.121768  4        1.014467\n\n\nThis linear model regression investigates the relationship between total income from the previous year Total_Income_Prev_Year and multiple covariates (Race, Sex, Marital Status). While several predictors show statistical significance — notably Marital_StatusMarried, SexMale, and RaceNon-Black/Non-Hispanic — the adj. R-squared value is only ~0.09651, indicating that this model explains just under 10% of the variance in income. The low explanatory power suggests more testing, additional variables, and transformations may be required. When using plot(model) earlier (not pictured here), our diagnostic plots (Q-Q plot for instance) suggested that the residuals might not be following a normal distribution, which we need to consider for our next iterations of the model. As of now, multicollinearity doesn’t seem to be an issue given VIF values close to 1, but further refinement of the model is needed before making any conclusive interpretations."
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html",
    "title": "Blog Post #1",
    "section": "",
    "text": "This is DataDetectives’ first blog post, where we describe potential datasets to use for our final project.\n\n\n\n130 rows x 20 columns | U.S. Census Bureau’s 2023 Survey of Income and Program Participation (SIPP) | DataSet\n\nThe data is well organized but it is not in tsv/csv format so it will have to be converted before being loaded and cleaned. Questions we hope to address: What is the distribution of household wealth across different demographic and economic groups?, How do asset ownership and debt-holding rates vary among households?, What insights can be drawn about economic well-being based on household wealth variations? Some foreseen challenges: It may be difficult to determine whether factors like education and income cause differences in wealth or are merely associated with them, Variability in how households report asset values and debt could introduce inconsistencies in wealth calculations, and The dataset reflects household wealth at the end of 2022, a period influenced by inflation, market fluctuations, and post-pandemic recovery, which could affect interpretability.\n\n\n\n\n253 columns x 16384 rows | The U.S. Geological Survey | DataSet\n\nWe hope to use groundwater consumption and withdrawal data from the U.S. geological survey for our final project. This data outlines monthly public supply, irrigation, and thermoelectric water use from 2009 - 2020. The dataset, which comes from a federal source, already comes in a CSV file, so it is easy to clean, read, and manage. With this information, we hope to address the question: Do different communities’ drinking water lead to health disparities? However, this leads to the challenges with this particular dataset. On its own, the dataset doesn’t provide information that can answer a question related to racial disparities. In order to answer our questions we would need to collect US Census Data, which will give us demographics (age/race) for the particular year we decide to evaluate. While census data is robust, we will be able to filter the information to better support this initial dataset.\n\n\n\n\nMany columns and rows | Bureau of Labor Service | DataSet\n\nThe NLSY is a cohort of surveys administered to the same group of several thousand respondents over decades. The one we want to look at starts from 1979 and goes on until 2022, with the 1979 respondents being aged 14 to 18. Each survey, they are asked various questions from socioeconomic status to health and family size. Due to the breadth of the data available, there is an online data selection tool that allows for specific columns to be exported as CSV files for convenient use, and there are thousands of variables to choose from. Some questions we could answer are: what factors contribute most significantly to upward economic mobility over time? How do employment trends and income differ by gender, race, and education level? Are there racial disparities on long-term income and wealth? Visualization might be a bit difficult directly and it would likely require us to look at proportions instead. However, it’s nice that there is so much available content we could look at."
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-1---wealth-of-households-2022",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-1---wealth-of-households-2022",
    "title": "Blog Post #1",
    "section": "",
    "text": "130 rows x 20 columns | U.S. Census Bureau’s 2023 Survey of Income and Program Participation (SIPP) | DataSet\n\nThe data is well organized but it is not in tsv/csv format so it will have to be converted before being loaded and cleaned. Questions we hope to address: What is the distribution of household wealth across different demographic and economic groups?, How do asset ownership and debt-holding rates vary among households?, What insights can be drawn about economic well-being based on household wealth variations? Some foreseen challenges: It may be difficult to determine whether factors like education and income cause differences in wealth or are merely associated with them, Variability in how households report asset values and debt could introduce inconsistencies in wealth calculations, and The dataset reflects household wealth at the end of 2022, a period influenced by inflation, market fluctuations, and post-pandemic recovery, which could affect interpretability."
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-2---public-water-use",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-2---public-water-use",
    "title": "Blog Post #1",
    "section": "",
    "text": "253 columns x 16384 rows | The U.S. Geological Survey | DataSet\n\nWe hope to use groundwater consumption and withdrawal data from the U.S. geological survey for our final project. This data outlines monthly public supply, irrigation, and thermoelectric water use from 2009 - 2020. The dataset, which comes from a federal source, already comes in a CSV file, so it is easy to clean, read, and manage. With this information, we hope to address the question: Do different communities’ drinking water lead to health disparities? However, this leads to the challenges with this particular dataset. On its own, the dataset doesn’t provide information that can answer a question related to racial disparities. In order to answer our questions we would need to collect US Census Data, which will give us demographics (age/race) for the particular year we decide to evaluate. While census data is robust, we will be able to filter the information to better support this initial dataset."
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-3---national-longitudinal-survey-of-youth",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-3---national-longitudinal-survey-of-youth",
    "title": "Blog Post #1",
    "section": "",
    "text": "Many columns and rows | Bureau of Labor Service | DataSet\n\nThe NLSY is a cohort of surveys administered to the same group of several thousand respondents over decades. The one we want to look at starts from 1979 and goes on until 2022, with the 1979 respondents being aged 14 to 18. Each survey, they are asked various questions from socioeconomic status to health and family size. Due to the breadth of the data available, there is an online data selection tool that allows for specific columns to be exported as CSV files for convenient use, and there are thousands of variables to choose from. Some questions we could answer are: what factors contribute most significantly to upward economic mobility over time? How do employment trends and income differ by gender, race, and education level? Are there racial disparities on long-term income and wealth? Visualization might be a bit difficult directly and it would likely require us to look at proportions instead. However, it’s nice that there is so much available content we could look at."
  },
  {
    "objectID": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "href": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "title": "Blog Post #2",
    "section": "",
    "text": "Blog post 2\nThis is DataDetectives’ second blog post, where we dive into detail regarding one of the data sets we looked at in the past blog post.\n\nExploring Economic and Health Trends with the National Longitudinal Survey of Youth\nAfter much discussion, our team has selected Dataset #3, the National Longitudinal Survey of Youth (NLSY79), for our analysis. This dataset, managed by the Bureau of Labor Service, tracks thousands of respondents over several decades, beginning in 1979 when they were aged 14 to 18. The survey covers a vast range of socioeconomic factors, including employment trends, income disparities, and family dynamics. With thousands of variables available, we plan to utilize the online data selection tool to extract key information that can help answer important questions: What factors contribute to upward economic mobility? How do income and employment differ across gender, race, and education levels? Are there persistent racial disparities in long-term wealth accumulation? While the depth of the dataset is impressive, visualizing trends may require proportion-based analysis rather than direct numerical comparisons due to the complexity of the data.\nTo ensure accurate and meaningful insights, we have outlined a structured data parsing and cleaning approach. Since the dataset primarily consists of numeric codes representing different categorical values, our first step is decoding these into readable formats. Next, we will filter out non-interviewed individuals—those who have dropped out of the study due to reasons such as death or refusal to participate. Additionally, we will focus on data from the year 2010, removing records from 2009 and 2011 to streamline our analysis. This approach allows us to concentrate on a single pivotal year, avoiding potential inconsistencies introduced by overlapping time periods.\nGiven the project’s requirements, we are also considering integrating a second dataset to enhance our analysis. One promising option is incorporating health-related data, including health conditions, insurance coverage, and healthcare usage. This could allow us to examine correlations between employment history and healthcare access, particularly in the wake of significant events such as the 2008 financial crisis and the Affordable Care Act’s early years. Additionally, with the swine flu epidemic occurring in 2009, analyzing its impact on healthcare trends in 2010 could yield valuable insights. By combining economic and health-related factors, we aim to develop a more comprehensive understanding of how socioeconomic conditions influence long-term well-being."
  },
  {
    "objectID": "posts/2025-04-21-blog-post-7/blog-post-7.html",
    "href": "posts/2025-04-21-blog-post-7/blog-post-7.html",
    "title": "Blog Post #7",
    "section": "",
    "text": "Blog post 7\n\n\nOur Interactive - Design Goals\nWe plan to use an interactive to analyze the impact of the 2008 recession on different groups of people. We plan to allow users to select one or more features like race, marital status, income, etc. to compare.\nOne idea– a user can start by viewing a line chart showing income trends across education levels from 2000 to 2010, then zoom into subgroups defined by race, gender, or marital status. We can include tooltips, highlighted callouts, brief annotations, or even letting users “find themselves” with their own information.\nAnother idea– we can create two maps graphics with identical statistics that the user changes, but one from 2008 and another from 2010. This way, the user can see the impact of the recession on different racial groups in different US regions.\n\n\nOur Interactive - Progress so Far\nSo far, most of our intial data cleaning and combining is finally complete. From last week, we combined our two datasets to create a more holistic view of the macroeconomic trends in America during the 2008 recession.\nNow, we plan on using these datasets to create the interactive feature using shiny or quarto::interactive().\n\n\nOur Interactive - Next Steps\nStep 1: Finalize the UI design and incorporate dropdowns for user selection. Step 2: Implement components to adjust plots/statistics based on user input with R script. This step will liekly take the longest and involve the most debugging. Step 3: Write captions and annotations to help guide interpretation. Step 4: Test the app with users to ensure it’s intuitive and engaging."
  },
  {
    "objectID": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "href": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "title": "Blog Post #5",
    "section": "",
    "text": "For this blog post, we sought out to add a new dataset to our project. After discussing, we decided on the interest rates archive of the Department of Treasury (Link: https://home.treasury.gov/interest-rates-data-csv-archive). The data is presented daily, and to join with our existing data nicely, we thought of looking at the average interest rate each year and joining it with yearly available data in our existing NLSY data, mainly income. We plan on seeing if there’s any relationship between annual interest rates and income stratified by race. Up until this point, we had only looked at income data from 2010, so we will need to download income data for years 2000-2020 and expand the NLSY dataset. This should not be an issue, but we will have to slightly modify the cleaning code to account for the new years. Then, we downloaded Daily Treasury Long-Term Rates from the website. Since these are relatively stable as long term rates, aggregating over a year and averaging should give us a good idea of the interest rate for that year. Overall, this should be a good way to see if broader macroeconomic conditions impact the financial wellbeing of people different backgrounds in different ways."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "title": "Blog Post #6",
    "section": "",
    "text": "This is DataDetectives’ sixth blog post, where we begin to load and clean our second dataset\n\n\nIn this analysis, we are working with “long-term-rates-2000-2010.csv”. As explained in our previous blog post, this set contains daily records of U.S. long-term interest rates between 2000 and 2010. The two main variables of interest were:LT COMPOSITE (&gt;10 Yrs) and TREASURY 20-Yr CMT\n\n\n\nWe began by importing the dataset using the read_csv() function from the readr package in the tidyverse. The Date column, originally in character format, was converted to proper Date objects using lubridate::mdy(). We then extracted the year from each date to enable grouping by year.\nTo understand yearly trends in interest rates, we grouped the data by Year and computed a comprehensive set of summary statistics for each of the two rate columns including mean, minimum, maximum, standard deviation, median, and mode. In order to create a column for mode I created a custom get_mode() function to handle mode calculation, since R does not provide a built-in mode() for numeric data in a tidy pipeline.\n\n\n\nThen, we first added additional income years to cover 2000-2010 as opposed to just 2010. Afterwards, we joined the interest rate average of each year with the corresponding two years - so 2001 and 2002 went into 2002, 2003 and 2004 went into 2004, and so on. By doing this, we were able to join the interest rate data into our survey data in an efficient and complementary way. With that, we also did a rudimentary graph looking at income over time for each racial group alongside the interest rate each year, which can be seen in analysis.qmd.\n\n\n\nWe are considering issues with income lag due to various external factors compounding with macroeconomic factors in the US from 2008-2010. Therefore, we may also have to include R script to load and clean the same numbers for interest rates in 2011-2020."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#our-dataset",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#our-dataset",
    "title": "Blog Post #6",
    "section": "",
    "text": "In this analysis, we are working with “long-term-rates-2000-2010.csv”. As explained in our previous blog post, this set contains daily records of U.S. long-term interest rates between 2000 and 2010. The two main variables of interest were:LT COMPOSITE (&gt;10 Yrs) and TREASURY 20-Yr CMT"
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#loading-and-cleaning",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#loading-and-cleaning",
    "title": "Blog Post #6",
    "section": "",
    "text": "We began by importing the dataset using the read_csv() function from the readr package in the tidyverse. The Date column, originally in character format, was converted to proper Date objects using lubridate::mdy(). We then extracted the year from each date to enable grouping by year.\nTo understand yearly trends in interest rates, we grouped the data by Year and computed a comprehensive set of summary statistics for each of the two rate columns including mean, minimum, maximum, standard deviation, median, and mode. In order to create a column for mode I created a custom get_mode() function to handle mode calculation, since R does not provide a built-in mode() for numeric data in a tidy pipeline."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#modifying-the-original-set",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#modifying-the-original-set",
    "title": "Blog Post #6",
    "section": "",
    "text": "Then, we first added additional income years to cover 2000-2010 as opposed to just 2010. Afterwards, we joined the interest rate average of each year with the corresponding two years - so 2001 and 2002 went into 2002, 2003 and 2004 went into 2004, and so on. By doing this, we were able to join the interest rate data into our survey data in an efficient and complementary way. With that, we also did a rudimentary graph looking at income over time for each racial group alongside the interest rate each year, which can be seen in analysis.qmd."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#future-plans",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#future-plans",
    "title": "Blog Post #6",
    "section": "",
    "text": "We are considering issues with income lag due to various external factors compounding with macroeconomic factors in the US from 2008-2010. Therefore, we may also have to include R script to load and clean the same numbers for interest rates in 2011-2020."
  },
  {
    "objectID": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "href": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Here are some basic figures we created after cleaning, these are meant to show a quick breakdown of some of the variables we cleaned.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\n#count of races that participated \nggplot(nls_data_clean, aes(x = Race, fill = Sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Participant Count by Race and Sex\", x = \"Race\", y = \"Count\")\n\n\n\n\n\n\n\n#count of different marital statuses found in survey\nggplot(nls_data_clean, aes(x = Marital_Status)) +\n  geom_bar(fill = \"orchid\") +\n  labs(title = \"Marital Status Distribution\", x = \"Marital Status\", y = \"Count\")\n\n\n\n\n\n\n\n\nOur data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove “non-interview” data points that we can’t compare across years, and decode integer values representing race, grade, etc. For this process, we primarily depended on the tidyverse library and other methods discussed in lecture.\nTo begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes. Our next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values.\nAnother challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = “black”, 2 = “white”, etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns.\nNow that our initial dataset is clean we plan to create more visuals to analyze this set, determine our next dataset to combine with this one, choose which additional data from this source to include (and clean of course)."
  },
  {
    "objectID": "posts/2025-03-24-blog-post-3/blog-post-3.html#data-cleaning",
    "href": "posts/2025-03-24-blog-post-3/blog-post-3.html#data-cleaning",
    "title": "Blog Post 3",
    "section": "",
    "text": "Here are some basic figures we created after cleaning, these are meant to show a quick breakdown of some of the variables we cleaned.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\n#count of races that participated \nggplot(nls_data_clean, aes(x = Race, fill = Sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Participant Count by Race and Sex\", x = \"Race\", y = \"Count\")\n\n\n\n\n\n\n\n#count of different marital statuses found in survey\nggplot(nls_data_clean, aes(x = Marital_Status)) +\n  geom_bar(fill = \"orchid\") +\n  labs(title = \"Marital Status Distribution\", x = \"Marital Status\", y = \"Count\")\n\n\n\n\n\n\n\n\nOur data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove “non-interview” data points that we can’t compare across years, and decode integer values representing race, grade, etc. For this process, we primarily depended on the tidyverse library and other methods discussed in lecture.\nTo begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes. Our next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values.\nAnother challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = “black”, 2 = “white”, etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns.\nNow that our initial dataset is clean we plan to create more visuals to analyze this set, determine our next dataset to combine with this one, choose which additional data from this source to include (and clean of course)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a website for the final project for MA415 Data Science with R by Team Data Detectives.\nThe members of this team are below:"
  },
  {
    "objectID": "about.html#chandini-toleti",
    "href": "about.html#chandini-toleti",
    "title": "About",
    "section": "Chandini Toleti",
    "text": "Chandini Toleti\nChandini is majoring in Mathematics and Computer Science at Boston University, Class of 2026’\nGithub"
  },
  {
    "objectID": "about.html#ranbir",
    "href": "about.html#ranbir",
    "title": "About",
    "section": "Ranbir",
    "text": "Ranbir\nRanbir (Ran) is majoring in Data Science at Boston University, Class of 2026."
  },
  {
    "objectID": "about.html#patrio-marcus",
    "href": "about.html#patrio-marcus",
    "title": "About",
    "section": "Patrio Marcus",
    "text": "Patrio Marcus\nRio (Class of ’26) is majoring in mathematics on the statistics concentration at Boston University. https://github.com/Patrio-Marcus"
  },
  {
    "objectID": "about.html#aretha-mcdonald",
    "href": "about.html#aretha-mcdonald",
    "title": "About",
    "section": "Aretha McDonald",
    "text": "Aretha McDonald\nAretha is completing a Master’s program in Bioinformatics at Boston University, Class of 2025."
  },
  {
    "objectID": "about.html#emir",
    "href": "about.html#emir",
    "title": "About",
    "section": "Emir",
    "text": "Emir\nEmir is a sophomore majoring in Economics & Mathematics alongside Data Science at Boston University.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]