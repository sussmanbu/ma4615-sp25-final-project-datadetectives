[
  {
    "objectID": "posts/2025-04-30-blog-post-8/blog-post-8.html",
    "href": "posts/2025-04-30-blog-post-8/blog-post-8.html",
    "title": "Blog Post #8",
    "section": "",
    "text": "Blog post 8\n#Intro In our last blog post, we discussed adjusting plots/stats and incorporating additional design elements; so far we’ve been able to build out our shiny element –&gt; a map which shows the national income average for the years 2006, 2008 and 2010. Additionally, it allows users to filter these income results by region selected on map and race.\n#Continuing Exploratory Data Analysis Currently, we have explored the data through visualizations we thought could provide critical insights into income disparities by gender, race, and region, as well as model diagnostics for regression analysis. So far we’ve learned: * Men earn more than women across all regions, with the South and West showing the highest disparity (including more zero-income women). * A significant proportion of women (especially in the South/West) report $0 income, which may indicate:\n\nUnemployment (higher job losses in female-dominated sectors during the recession).\n\n\nUnderreporting (e.g., informal work, unpaid caregiving).\n\n\nSelection bias (e.g., survey non-response from low-income women).\n\nExtreme high-income outliers skewed initial models, requiring careful treatment.\nWe could continue to explore this side of the data by comparing Black women in the South vs. West—did one group fare worse post-recession?\nWe could compare pre/post recession distribution - did the recession widen the income gap?\n\nAdditionally, we can look at things like:\n\nDid single mothers face sharper declines due to childcare costs?\nPlot income growth against mortgage/loan rates—did lower rates help high-income groups refinance debt?\n\n#Thesis Falling interest rates during the 2008 recession provided financial relief for high-income earners but failed to prevent income declines for Black households, reflecting systemic inequities in economic recovery. Hispanic households experienced relative resilience, suggesting structural differences in employment stability and access to credit.\n#Visualizations and Tables\nNext steps for Improving Visualizations and Tables: 1. Titles & Captions - use more descriptive titles, sdd subtitles/captions explaining data sources, time periods, or key observations.\n\nAnnotation & Highlights - Label key groups (e.g., “Zero-income women in South”), add recession shading, and emphasize disparities with arrows/text\nThemes & Aesthetics - Use theme_pubr() for clean, publication-ready layouts and rotate x-labels if long."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html",
    "title": "Blog Post #6",
    "section": "",
    "text": "This is DataDetectives’ sixth blog post, where we begin to load and clean our second dataset\n\n\nIn this analysis, we are working with “long-term-rates-2000-2010.csv”. As explained in our previous blog post, this set contains daily records of U.S. long-term interest rates between 2000 and 2010. The two main variables of interest were:LT COMPOSITE (&gt;10 Yrs) and TREASURY 20-Yr CMT\n\n\n\nWe began by importing the dataset using the read_csv() function from the readr package in the tidyverse. The Date column, originally in character format, was converted to proper Date objects using lubridate::mdy(). We then extracted the year from each date to enable grouping by year.\nTo understand yearly trends in interest rates, we grouped the data by Year and computed a comprehensive set of summary statistics for each of the two rate columns including mean, minimum, maximum, standard deviation, median, and mode. In order to create a column for mode I created a custom get_mode() function to handle mode calculation, since R does not provide a built-in mode() for numeric data in a tidy pipeline.\n\n\n\nThen, we first added additional income years to cover 2000-2010 as opposed to just 2010. Afterwards, we joined the interest rate average of each year with the corresponding two years - so 2001 and 2002 went into 2002, 2003 and 2004 went into 2004, and so on. By doing this, we were able to join the interest rate data into our survey data in an efficient and complementary way. With that, we also did a rudimentary graph looking at income over time for each racial group alongside the interest rate each year, which can be seen in analysis.qmd.\n\n\n\nWe are considering issues with income lag due to various external factors compounding with macroeconomic factors in the US from 2008-2010. Therefore, we may also have to include R script to load and clean the same numbers for interest rates in 2011-2020."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#our-dataset",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#our-dataset",
    "title": "Blog Post #6",
    "section": "",
    "text": "In this analysis, we are working with “long-term-rates-2000-2010.csv”. As explained in our previous blog post, this set contains daily records of U.S. long-term interest rates between 2000 and 2010. The two main variables of interest were:LT COMPOSITE (&gt;10 Yrs) and TREASURY 20-Yr CMT"
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#loading-and-cleaning",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#loading-and-cleaning",
    "title": "Blog Post #6",
    "section": "",
    "text": "We began by importing the dataset using the read_csv() function from the readr package in the tidyverse. The Date column, originally in character format, was converted to proper Date objects using lubridate::mdy(). We then extracted the year from each date to enable grouping by year.\nTo understand yearly trends in interest rates, we grouped the data by Year and computed a comprehensive set of summary statistics for each of the two rate columns including mean, minimum, maximum, standard deviation, median, and mode. In order to create a column for mode I created a custom get_mode() function to handle mode calculation, since R does not provide a built-in mode() for numeric data in a tidy pipeline."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#modifying-the-original-set",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#modifying-the-original-set",
    "title": "Blog Post #6",
    "section": "",
    "text": "Then, we first added additional income years to cover 2000-2010 as opposed to just 2010. Afterwards, we joined the interest rate average of each year with the corresponding two years - so 2001 and 2002 went into 2002, 2003 and 2004 went into 2004, and so on. By doing this, we were able to join the interest rate data into our survey data in an efficient and complementary way. With that, we also did a rudimentary graph looking at income over time for each racial group alongside the interest rate each year, which can be seen in analysis.qmd."
  },
  {
    "objectID": "posts/2025-04-14-blog-post-6/blog-post-6.html#future-plans",
    "href": "posts/2025-04-14-blog-post-6/blog-post-6.html#future-plans",
    "title": "Blog Post #6",
    "section": "",
    "text": "We are considering issues with income lag due to various external factors compounding with macroeconomic factors in the US from 2008-2010. Therefore, we may also have to include R script to load and clean the same numbers for interest rates in 2011-2020."
  },
  {
    "objectID": "posts/2025-03-28-blog-post-4/blog-post-4.html",
    "href": "posts/2025-03-28-blog-post-4/blog-post-4.html",
    "title": "Blog Post #4",
    "section": "",
    "text": "For this blog post some members of our team each did exploratory analysis into our Data.\n\nChandini’s Graph\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\n#define correct order of education levels(neeeded for next query)\neducation_levels &lt;- c(\n  \"1ST GRADE\", \"2ND GRADE\", \"3RD GRADE\", \"4TH GRADE\", \"5TH GRADE\", \"6TH GRADE\", \n  \"7TH GRADE\", \"8TH GRADE\", \"9TH GRADE\", \"10TH GRADE\", \"11TH GRADE\", \"12TH GRADE\", \n  \"1ST YEAR COLLEGE\", \"2ND YEAR COLLEGE\", \"3RD YEAR COLLEGE\", \"4TH YEAR COLLEGE\", \n  \"5TH YEAR COLLEGE\", \"6TH YEAR COLLEGE\", \"7TH YEAR COLLEGE\", \"8TH YEAR COLLEGE OR MORE\",\n  \"UNGRADED\", NA\n)\n\n#mutate to order based on most education completed \nnls_data_clean &lt;- nls_data_clean %&gt;%\n  mutate(Highest_Grade_Completed = factor(Highest_Grade_Completed, \n                                          levels = education_levels,\n                                          ordered = TRUE))\n#Calcualte average mean_income for each grade category\nincome_by_grade &lt;- nls_data_clean %&gt;%\n  group_by(Highest_Grade_Completed) %&gt;%\n  summarise(mean_income = mean(Total_Income_Prev_Year, na.rm = TRUE)) %&gt;%\n  filter(!is.na(Highest_Grade_Completed))\n\n#income vs highest grade plotted with ggplot\nggplot(income_by_grade, aes(x = Highest_Grade_Completed, y = mean_income, group = 1)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(color = \"darkred\", size = 2) +\n  labs(title = \"Average Income by Highest Grade Completed\",\n       x = \"Highest Grade Completed\",\n       y = \"Average Income\") +\n  scale_y_continuous(labels = label_comma()) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\nHere, we analyze the relationship between highest grade completed and average income. In order to create this plot, we had to order each grade group and calculate mean incomes for each one. Finally, we discovered the positive relationship we predicted. This data shows that the more education people completed, the higher their income. However, we also see the presence of outliers impacting the data’s skew. Regardless, these variables will be useful for future data analysis when we combine our data with recession data. Hopefully, we can compare these variables to recession information and compare the relationship between them before and after.\n\n\nAretha’s Graph\n\nlibrary(scales)\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\nnls_data_clean &lt;- nls_data_clean %&gt;%\n  filter(!is.na(Region))\n\n# Regional Variation in Gender Income Gap\nggplot(nls_data_clean, aes(x = Region, y = Total_Income_Prev_Year, fill = Sex)) +\n  geom_boxplot(outlier.alpha = 0.5, outlier.size = 1) +\n  labs(title = \"Income Distribution by Region and Gender\",\n       x = \"Region\", \n       y = \"Total Income (Previous Year)\", \n       fill = \"Gender\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 359 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThis box plot explores the depth of the relationship between income, gender and region to examine whether there are any disparities between genders. In order to plot this graph, I had to add the ‘region’ column which describes which region of the US participants reside in - this usually has an impact on base salaries and living expenses. We believe this data column will become increasingly useful in further analysis as we look to add recession data as our 2nd dataset. We can potentially look how certain regions were impacted (based on income/debt) in comparison to others.\n\n\nRan’s Model\n\nlibrary(scales)\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\nnls_data_clean_filtered &lt;- nls_data_clean %&gt;%\n  filter(!is.na(Total_Income_Prev_Year), is.finite(Total_Income_Prev_Year))\n\nset.seed(1234)\nmodel = lm(Total_Income_Prev_Year ~ Race + Sex + Marital_Status, data = nls_data_clean_filtered)\nmodel_data = model.frame(model)\nmodel_data$Predicted = predict(model)\nsummary(model)\n\n\nCall:\nlm(formula = Total_Income_Prev_Year ~ Race + Sex + Marital_Status, \n    data = nls_data_clean_filtered)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-63483 -28742  -9317  14905 295913 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    16411       1617  10.151  &lt; 2e-16 ***\nRaceHispanic                    3986       1670   2.387  0.01701 *  \nRaceNon-Black/Non-Hispanic     14684       1369  10.725  &lt; 2e-16 ***\nSexMale                        19741       1140  17.315  &lt; 2e-16 ***\nMarital_StatusMarried          12647       1470   8.605  &lt; 2e-16 ***\nMarital_StatusNever Married    -5821       1899  -3.066  0.00218 ** \nMarital_StatusSeparated        -8236       2808  -2.933  0.00337 ** \nMarital_StatusWidowed          -8836       4206  -2.101  0.03566 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48050 on 7187 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.09739,   Adjusted R-squared:  0.09651 \nF-statistic: 110.8 on 7 and 7187 DF,  p-value: &lt; 2.2e-16\n\nvif(model)\n\n                   GVIF Df GVIF^(1/(2*Df))\nRace           1.108749  2        1.026144\nSex            1.011891  1        1.005928\nMarital_Status 1.121768  4        1.014467\n\n\nThis linear model regression investigates the relationship between total income from the previous year Total_Income_Prev_Year and multiple covariates (Race, Sex, Marital Status). While several predictors show statistical significance — notably Marital_StatusMarried, SexMale, and RaceNon-Black/Non-Hispanic — the adj. R-squared value is only ~0.09651, indicating that this model explains just under 10% of the variance in income. The low explanatory power suggests more testing, additional variables, and transformations may be required. When using plot(model) earlier (not pictured here), our diagnostic plots (Q-Q plot for instance) suggested that the residuals might not be following a normal distribution, which we need to consider for our next iterations of the model. As of now, multicollinearity doesn’t seem to be an issue given VIF values close to 1, but further refinement of the model is needed before making any conclusive interpretations."
  },
  {
    "objectID": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "href": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "title": "Blog Post #2",
    "section": "",
    "text": "Blog post 2\nThis is DataDetectives’ second blog post, where we dive into detail regarding one of the data sets we looked at in the past blog post.\n\nExploring Economic and Health Trends with the National Longitudinal Survey of Youth\nAfter much discussion, our team has selected Dataset #3, the National Longitudinal Survey of Youth (NLSY79), for our analysis. This dataset, managed by the Bureau of Labor Service, tracks thousands of respondents over several decades, beginning in 1979 when they were aged 14 to 18. The survey covers a vast range of socioeconomic factors, including employment trends, income disparities, and family dynamics. With thousands of variables available, we plan to utilize the online data selection tool to extract key information that can help answer important questions: What factors contribute to upward economic mobility? How do income and employment differ across gender, race, and education levels? Are there persistent racial disparities in long-term wealth accumulation? While the depth of the dataset is impressive, visualizing trends may require proportion-based analysis rather than direct numerical comparisons due to the complexity of the data.\nTo ensure accurate and meaningful insights, we have outlined a structured data parsing and cleaning approach. Since the dataset primarily consists of numeric codes representing different categorical values, our first step is decoding these into readable formats. Next, we will filter out non-interviewed individuals—those who have dropped out of the study due to reasons such as death or refusal to participate. Additionally, we will focus on data from the year 2010, removing records from 2009 and 2011 to streamline our analysis. This approach allows us to concentrate on a single pivotal year, avoiding potential inconsistencies introduced by overlapping time periods.\nGiven the project’s requirements, we are also considering integrating a second dataset to enhance our analysis. One promising option is incorporating health-related data, including health conditions, insurance coverage, and healthcare usage. This could allow us to examine correlations between employment history and healthcare access, particularly in the wake of significant events such as the 2008 financial crisis and the Affordable Care Act’s early years. Additionally, with the swine flu epidemic occurring in 2009, analyzing its impact on healthcare trends in 2010 could yield valuable insights. By combining economic and health-related factors, we aim to develop a more comprehensive understanding of how socioeconomic conditions influence long-term well-being."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Cost of Crisis: Economic Inequality in the Wake of U.S. Macroeconomic Shocks",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #8\n\n\n\n\n\nThis is Data Detectives’ 8th blog post, where we explain our thesis, dicuss how we have continued our exploratory data analysis, and describe how we plan to polish our visualizations and tables. \n\n\n\n\n\nApr 30, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #7\n\n\n\n\n\nThis is Data Detectives’ 7th blog post, where we describe our thoughts and progress on the interactive for our final project \n\n\n\n\n\nApr 21, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #6\n\n\nExploratory Data Analysis\n\n\nThis is data detectives’ sixth blog post, where we continue our work. \n\n\n\n\n\nApr 14, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #5\n\n\nExploratory Data Analysis\n\n\nThis is data detectives’ fifth blog post, where we add a new data set about interest rates. \n\n\n\n\n\nApr 7, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #4\n\n\nExploratory Data Analysis\n\n\nThis is data detectives’ fourth blog post, where we do some initial exploratory data analysis on our cleaned data set. \n\n\n\n\n\nMar 28, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3\n\n\nData Loading/Cleaning\n\n\nThis is Data Detectives’ third blog post where we further investigate, load, and clean our initial dataset. \n\n\n\n\n\nMar 24, 2025\n\n\nData Detectives\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #2\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nPatrio Marcus, Chandini Toleti, Ranbir, Aretha McDonald, Emir\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #1\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nPatrio Marcus, Chandini Toleti, Ranbir, Aretha McDonald, Emir\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Boom, Bust and Balance - Between The Lines of Income and Interest Rates",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\n\nIntroduction\nPicture this — you’re approaching a major crossroads in life. Maybe you’re saving for retirement, helping your kids through college, or finally feeling financially secure after decades of hard work. Then, almost overnight, the economy crumbles. Home values plummet, jobs vanish, and uncertainty becomes the new normal.\nThis was the reality for members of the 1979 National Longitudinal Survey of Youth cohort, who found themselves navigating one of the most severe economic downturns in modern history right at a critical point in their financial lives.\nOur aim is to show how the Great Recession of 2008 impact these members’ incomes. We want to show how falling interest rates shaped their financial decisions and opportunities. In the analysis ahead, we dive into these questions — using regression models to uncover how income and interest rate dynamics shifted from 2006 to 2010. Join us as we explore the lasting financial ripples of the crisis and the stories hidden within the numbers.\n\n\n\nInteractive\nBelow is our interactive dashboard - a map which shows the national income average for the years 2006, 2008 and 2010. Additionally, it allows users to filter these income results by region selected on map and race.\n\nWhat You Will Observe :\n\nOverall national income grew from 2006 to 2008 but stagnated slightly in 2010 (41,284 → 41,670)\n\n2.Non-Hispanic incomes followed a similar trend but remained higher than the national average\n\nBlack incomes rose until 2008 but declined slightly in 2010, suggesting a stronger recessionary impact\n\n4.Hispanic incomes saw steady growth, even post-recession (37,998 → 40,986)\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| echo: false \n#| standalone: true\n#| warning: false\n#| viewerHeight: 800\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(shinydashboard)\nlibrary(sf)\n\n# Load nls_no_outliers\nnls_no_outliers &lt;- read_csv(\"https://sussmanbu.github.io/ma4615-sp25-final-project-datadetectives/scripts/nls_no_outliers_shiny.csv\")\n\n# Pivot from long to wide format\nnls_no_outliers_wide &lt;- nls_no_outliers %&gt;%\n  pivot_wider(\n    id_cols = c(Case_ID, Sample_ID, Race, Sex, Highest_Grade_Completed, Marital_Status, Region, Age_2010),\n    names_from = Income_Year,\n    values_from = Income,\n    names_prefix = \"Income_\"\n  )\n\n# Load data\nnls_data_clean &lt;- nls_no_outliers_wide\n\n# Load US states GeoJSON\nus_states &lt;- read_rds(\"https://sussmanbu.github.io/ma4615-sp25-final-project-datadetectives/scripts/us_states.rds\")\n\n# Create region lookup\nregion_lookup &lt;- tibble(\n  state = tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \n                    \"rhode island\", \"connecticut\", \"new york\", \"new jersey\",\n                    \"pennsylvania\", \"ohio\", \"indiana\", \"illinois\", \"michigan\",\n                    \"wisconsin\", \"minnesota\", \"iowa\", \"missouri\", \"north dakota\",\n                    \"south dakota\", \"nebraska\", \"kansas\", \"delaware\", \"maryland\",\n                    \"virginia\", \"west virginia\", \"north carolina\", \"south carolina\",\n                    \"georgia\", \"florida\", \"kentucky\", \"tennessee\", \"alabama\",\n                    \"mississippi\", \"arkansas\", \"louisiana\", \"oklahoma\", \"texas\",\n                    \"montana\", \"wyoming\", \"colorado\", \"new mexico\", \"idaho\",\n                    \"utah\", \"arizona\", \"washington\", \"oregon\", \"nevada\",\n                    \"california\", \"alaska\", \"hawaii\")),\n  region = case_when(\n    state %in% tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \n                         \"rhode island\", \"connecticut\", \"new york\", \n                         \"new jersey\", \"pennsylvania\")) ~ \"North East\",\n    state %in% tolower(c(\"ohio\", \"indiana\", \"illinois\", \"michigan\", \"wisconsin\",\n                         \"minnesota\", \"iowa\", \"missouri\", \"north dakota\",\n                         \"south dakota\", \"nebraska\", \"kansas\")) ~ \"Central\",\n    state %in% tolower(c(\"delaware\", \"maryland\", \"virginia\", \"west virginia\",\n                         \"north carolina\", \"south carolina\", \"georgia\",\n                         \"florida\", \"kentucky\", \"tennessee\", \"alabama\",\n                         \"mississippi\", \"arkansas\", \"louisiana\", \"oklahoma\",\n                         \"texas\")) ~ \"South\",\n    TRUE ~ \"West\"\n  )\n)\n\nstate_to_region &lt;- region_lookup$region\nnames(state_to_region) &lt;- region_lookup$state\n\nus_states@data$region &lt;- state_to_region[tolower(us_states@data$name)]\nus_states &lt;- us_states[!is.na(us_states@data$region), ]\n\nus_states_sf &lt;- st_as_sf(us_states)\nus_states_sf$region &lt;- state_to_region[tolower(us_states_sf$name)]\nus_states_sf &lt;- us_states_sf %&gt;%\n  filter(!name %in% c(\"Alaska\", \"Hawaii\"))\nus_states_sf &lt;- st_make_valid(us_states_sf)\n\nregions_sf &lt;- us_states_sf %&gt;%\n  group_by(region) %&gt;%\n  summarize(geometry = st_union(geometry), .groups = \"drop\")\n\n# --- UI ---\nui &lt;- fluidPage(\n  titlePanel(\"Recession Impact Map (Regions)\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"selected_race\", \"Select Race:\",\n                  choices = c(\"- Select Race -\" = \"\", unique(nls_data_clean$Race))),\n      radioButtons(\"filter\", \"Filter Result Cards By:\",\n                   choices = c(\"Income by year\"), \n                   selected = \"Income by year\")\n    ),\n    mainPanel(\n  leafletOutput(\"map\", height = 400),\n  br(),\n  h4(\"Overall Country/Region Income\"),\n  fluidRow(\n    column(3, valueBoxOutput(\"card2006\", width = 12)),\n    column(1, div(style = \"font-size: 30px; text-align: center;\", icon(\"arrow-right\"))),\n    column(3, valueBoxOutput(\"card2008\", width = 12)),\n    column(1, div(style = \"font-size: 30px; text-align: center;\", icon(\"arrow-right\"))),\n    column(3, valueBoxOutput(\"card2010\", width = 12))\n  ),\n  br(),\n  conditionalPanel(\n    condition = \"input.selected_race != ''\",\n    h4(\"Selected Race Income\"),\n    fluidRow(\n      column(3, valueBoxOutput(\"card2006_race\", width = 12)),\n      column(1, div(style = \"font-size: 30px; text-align: center;\", icon(\"arrow-right\"))),\n      column(3, valueBoxOutput(\"card2008_race\", width = 12)),\n      column(1, div(style = \"font-size: 30px; text-align: center;\", icon(\"arrow-right\"))),\n      column(3, valueBoxOutput(\"card2010_race\", width = 12))\n    )\n  )\n)\n\n  )\n)\n\n# --- Server ---\nserver &lt;- function(input, output, session) {\n  \n  selected_region &lt;- reactiveVal(NULL)\n  \n  get_mean_value &lt;- function(year, type, race = NULL) {\n    region_name &lt;- selected_region()\n    data &lt;- nls_data_clean\n    \n    if (!is.null(race) && race != \"\") {\n      data &lt;- data %&gt;% filter(Race == race)\n    }\n    \n    if (type == \"Income by year\") {\n      if (is.null(region_name)) {\n        data %&gt;%\n          pull(paste0(\"Income_\", year)) %&gt;%\n          mean(na.rm = TRUE)\n      } else {\n        data %&gt;%\n          filter(Region == region_name) %&gt;%\n          pull(paste0(\"Income_\", year)) %&gt;%\n          mean(na.rm = TRUE)\n      }\n    }\n  }\n  \n  output$map &lt;- renderLeaflet({\n    leaflet(regions_sf) %&gt;%\n      addTiles() %&gt;%\n      addPolygons(\n        fillColor = ~case_when(\n          region == \"North East\" ~ \"#1f77b4\",\n          region == \"Central\" ~ \"#2ca02c\",\n          region == \"South\" ~ \"#ff7f0e\",\n          region == \"West\" ~ \"#d62728\",\n          TRUE ~ \"gray\"\n        ),\n        fillOpacity = 0.7,\n        color = \"white\",\n        weight = 1,\n        label = ~region,\n        layerId = ~region,\n        highlightOptions = highlightOptions(\n          weight = 5,\n          color = \"black\",\n          bringToFront = TRUE\n        )\n      )\n  })\n\n  observeEvent(input$map_shape_click, {\n    click_region &lt;- input$map_shape_click$id\n    if (!is.null(click_region)) {\n      data_region &lt;- case_when(\n        click_region == \"North East\" ~ \"Northeast\",\n        click_region == \"Central\" ~ \"North Central\",\n        TRUE ~ click_region\n      )\n      selected_region(data_region)\n    }\n  })\n  \n  render_income_card &lt;- function(year, color, race_specific = FALSE) {\n    renderValueBox({\n      race &lt;- if (race_specific && input$selected_race != \"\") input$selected_race else NULL\n      \n      if (race_specific && is.null(race)) {\n        NULL  # no box at all if race not selected\n      } else {\n        value &lt;- scales::dollar(round(get_mean_value(year, input$filter, race)))\n        \n        region_display &lt;- if (is.null(selected_region())) {\n          \"National Average\"\n        } else {\n          selected_region()\n        }\n        \n        subtitle &lt;- if (race_specific) {\n          paste0(input$selected_race, \" Income \", year)\n        } else {\n          paste0(\"Income \", year, \" - \", region_display)\n        }\n        \n        valueBox(value, subtitle, color = color)\n      }\n    })\n  }\n\n  output$card2006 &lt;- render_income_card(2006, \"blue\")\n  output$card2008 &lt;- render_income_card(2008, \"green\")\n  output$card2010 &lt;- render_income_card(2010, \"red\")\n  \n  output$card2006_race &lt;- render_income_card(2006, \"teal\", race_specific = TRUE)\n  output$card2008_race &lt;- render_income_card(2008, \"lime\", race_specific = TRUE)\n  output$card2010_race &lt;- render_income_card(2010, \"purple\", race_specific = TRUE)\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\n\nThe recession officially began in Dec 2007 and peaked in 2008-2009, yet most groups saw income growth in 2008—possibly due to lag effects or government stimulus e.g., 2008 Economic Stimulus Act. Additionally, average income rates declined over time in 2007/2008 - these lower rates could have boosted home refinancing for homes with stable incomes, reduces returns on savings, disproportionately affecting lower-income groups and eased creat access, but Black/Hispanic househould may still have faced higher loan rejection rates due to systemic biases.\n\n\n\nThesis\nFalling interest rates during the 2008 recession provided financial relief for high-income earners but failed to prevent income declines for Black households, reflecting systemic inequities in economic recovery. Hispanic households experienced relative resilience, suggesting structural differences in employment stability and access to credit.\n=======\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a website for the final project for MA415 Data Science with R by Team Data Detectives.\nThe members of this team are below:"
  },
  {
    "objectID": "about.html#chandini-toleti",
    "href": "about.html#chandini-toleti",
    "title": "About",
    "section": "Chandini Toleti",
    "text": "Chandini Toleti\nChandini is majoring in Mathematics and Computer Science at Boston University, Class of 2026’\nGithub"
  },
  {
    "objectID": "about.html#ranbir",
    "href": "about.html#ranbir",
    "title": "About",
    "section": "Ranbir",
    "text": "Ranbir\nRanbir (Ran) is majoring in Data Science at Boston University, Class of 2026."
  },
  {
    "objectID": "about.html#patrio-marcus",
    "href": "about.html#patrio-marcus",
    "title": "About",
    "section": "Patrio Marcus",
    "text": "Patrio Marcus\nRio (Class of ’26) is majoring in mathematics on the statistics concentration at Boston University. https://github.com/Patrio-Marcus"
  },
  {
    "objectID": "about.html#aretha-mcdonald",
    "href": "about.html#aretha-mcdonald",
    "title": "About",
    "section": "Aretha McDonald",
    "text": "Aretha McDonald\nAretha is completing a Master’s program in Bioinformatics at Boston University, Class of 2025."
  },
  {
    "objectID": "about.html#emir",
    "href": "about.html#emir",
    "title": "About",
    "section": "Emir",
    "text": "Emir\nEmir is a sophomore majoring in Economics & Mathematics alongside Data Science at Boston University.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html#introduction",
    "href": "analysis.html#introduction",
    "title": "Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nObjective and Research Questions\nThe goal of our analysis is to examine how macroeconomic trends in the aftermath of the 2008 recession, such as shifting interest rates, affected different racial and socioeconomic groups in the United States. We aim to answer some of the following general research questions:\n\nHow do income and employment differ across gender, race, and education levels during periods of economic downturn?\nWhat factors contribute to upward economic mobility?\nAre there persistent racial disparities in long-term wealth accumulation?\n\nTo limit the scope of the project, we decided to look at the age cohort of people born between 1957 and 1964, represented by the Bureau of Labor Statistics’ NLS 79 survey, who were working adults at the time of the recession. Looking at this population, some of the following questions for analysis arise:\n\nWhat effect did falling interest rates during the recession have on income? Can this relationship be modeled\nDid the recession exacerbate income inequality differently along racial, socioeconomic, regional, or other lines?\n\n\n\nMotivating Figures for Analysis\nThe following graphs explore some of the relationships between variables in our data to motivate this goal.\n\n\n\n\n\n\n\n\n\nThese boxplots explore the difference in income along the lines of age, race, and sex. Across most age and race groups, men in the dataset tend to make more money than women. The pay gap is especially pronounced among Non-Black / Non-Hispanic people. These boxplots indicate that there is a higher proportion of women who do not make any income. The higher number of zero-income values causes some issues for analysis to be accounted for during outlier removal. Our analysis aims to examine how the differences observed in income between groups observed in these boxplots were impacted by the recession and macroeconomic trends.\n\n\n\n\n\n\n\n\n\nThis graph shows how interest rates declined over the course of the 2000s, and juxtaposes this trend with income for Black, Hispanic, and Non-Black / Non-Hispanic people. Whereas incomes for people within the first two hroups saw a decline in average income, Non-Black / Non-Hispanic peolpe saw a slight increase, further indicating that macroeconomic trends such as indicated through interest rates might have different impacts on different demographic groups."
  },
  {
    "objectID": "analysis.html#modeling-and-inference",
    "href": "analysis.html#modeling-and-inference",
    "title": "Analysis",
    "section": "Modeling and Inference",
    "text": "Modeling and Inference\nFirst tried with outliers and had bad figures, and we determined there were severe outliers impacting our model.\n\n\n\n\n\n\n\n\n\nWe therefore used R script like below to remove these and create a new dataset specifically for modeling.\nlibrary(tidyverse)\n\n# Load  merged dataset\nnls_with_rates &lt;- read_csv(\"dataset/nls_with_rates_full.csv\")\n\n# Step 1: Calculate IQR boundaries\nQ1 &lt;- quantile(nls_with_rates$Income, 0.25, na.rm = TRUE)\nQ3 &lt;- quantile(nls_with_rates$Income, 0.75, na.rm = TRUE)\nIQR_value &lt;- Q3 - Q1\n\nlower_bound &lt;- Q1 - 0.24 * IQR_value #lots of 0 (n/a) data\nupper_bound &lt;- Q3 + 1.5 * IQR_value\n\n# Step 2: Filter out the outliers\nnls_no_outliers &lt;- nls_with_rates %&gt;%\n  filter(Income &gt;= lower_bound & Income &lt;= upper_bound)\n\n# Step 3: Save it\nwrite_csv(nls_no_outliers, \"dataset/nls_no_outliers.csv\")\nAfter cleaning the data with the script, we began creating our model. After reading the “No Outliers” dataset, we filtered the data by the income years of interest (2006, 2008, and 2010). We also transformed the data by changing the education levels to binned groups (“Less than HS”, “High School”, “Some College”, and “College+” (College+ standing for college graduate or more)) and baseline of “Never Married” for Marital Status. The remaining baselines were determined through alphabetical order automatically through the model (which has been addressed in our conclusions).\nAfter omitting NA values for the bins, we utilized lm to create a linear model, testing covariates of Race, Sex, Marital Status, Age_2010 (Age at Year 2010), Region, edu_bin (education group bins), and avg_LT_rate (average long-term interest rates) and received the following model summary statistics.\n\n\n\nCoefficients\n\n\nterm\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\nSignif.\n\n\n\n\n(Intercept)\n19662.672\n5425.858\n3.624\n2.91e-04\n***\n\n\nRaceHispanic\n2807.627\n669.492\n4.194\n2.76e-05\n***\n\n\nRaceNon-Black/Non-Hispanic\n2993.485\n508.561\n5.886\n4.06e-09\n***\n\n\nSexMale\n14242.081\n410.012\n34.736\n&lt; 2e-16\n***\n\n\nMarital_StatusDivorced\n4408.501\n722.159\n6.105\n1.06e-09\n***\n\n\nMarital_StatusMarried\n6627.434\n627.593\n10.560\n&lt; 2e-16\n***\n\n\nMarital_StatusSeparated\n-1682.529\n1146.696\n-1.467\n1.42e-01\n\n\n\nMarital_StatusWidowed\n-677.400\n1637.284\n-0.414\n6.79e-01\n\n\n\nAge_2010\n-4.619\n91.514\n-0.050\n9.60e-01\n\n\n\nRegionNortheast\n4010.788\n679.436\n5.903\n3.66e-09\n***\n\n\nRegionSouth\n445.695\n528.300\n0.844\n3.99e-01\n\n\n\nRegionWest\n2824.512\n647.839\n4.360\n1.31e-05\n***\n\n\nedu_binHigh School\n10851.067\n1621.460\n6.692\n2.30e-11\n***\n\n\nedu_binSome College\n18365.587\n1646.517\n11.154\n&lt; 2e-16\n***\n\n\nedu_binCollege+\n29432.746\n1654.119\n17.794\n&lt; 2e-16\n***\n\n\navg_LT_rate\n-2523.204\n567.790\n-4.444\n8.91e-06\n***\n\n\n\n\n\n\n\n\nModel Fit Statistics\n\n\nResidual SE\nDegrees of Freedom\nR-squared\nAdjusted R-squared\nF-statistic\nP-Value\n\n\n\n\n22033\n11871\n0.2009\n0.1999\n199\n&lt; 2e-16\n\n\n\n\n\n\n\n\nOur model has a r-squared value of 0.2009, indicating that 20.09% of the variance in our model is explained by the model’s covariates. The adjusted r-squared value is slightly lower at 0.1999, meaning our model explains 19.99% of the variance in the model after adjusting for the number of predictors.\nThe F-statistic of 199 and p-value of &lt;2e-16 (extremely close to 0) confirms that the overall model is statistically significant; Atleast some of the covariates in the model explain our response variable Income. Only Age_2010 and RegionSouth fail to have a significant effect in explaining Income. The residual standard error of 22033 (roughly 0.5418 of mean income) indicates substantial noise exists in our model.\n\nBased on the metrics we observed, we observed our model’s diagnostic plots in the form of a residual vs. fitted plot and a Q-Q normality plot.\n\n\n\n\n\n\n\n\n\n\nThe residual vs. fitted values for our model reveals funneling, indicating moderate levels of heteroscedasticity that could bias the standard errors for our coefficients. This could distort our t-test statistics and p-values and undermining the actual significance of covariates in the model.\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing our normal Q-Q plot, most points follow along the dashed diagonal line very well; most residuals are approximately normally distributed.\nOn the ends, several points stray away, reflecting a few extreme (under and over) predictions and heavier tails than a normal Gaussian distribution. These results are expected as we only trimmed the most extreme outliers and because of the nature of our data (most people cluster around the middle of the distribution, but a few people earn very little while a few earn very large sums – disparity in income distribution).\n\nAnother consideration we made was to check for multicollinearity (for the precision of our coefficient estimates) by computing predictor-level GVIF values adjusted with degrees of freedom. All values fell below 1.5, with most covariates having GVIF values close to 1. No interaction terms were needed in our model, and our r-squared and adjusted r-squared values are not inflated by correlated predictors.\n\n\n\nPredictor-level Variance Inflation Factors\n\n\nPredictor\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nRace\n1.338842\n2\n1.075678\n\n\nSex\n1.028136\n1\n1.013970\n\n\nMarital_Status\n1.104040\n4\n1.012449\n\n\nAge_2010\n1.004665\n1\n1.002330\n\n\nRegion\n1.234429\n3\n1.035725\n\n\nedu_bin\n1.068939\n3\n1.011173\n\n\navg_LT_rate\n1.000200\n1\n1.000100\n\n\n\n\n\n\n\nAfter checking diagnostics and GVIFs:\n\nModerate heteroscedasticity existed (seen in residual vs. fitted plot).\nResiduals in our Q-Q plot were roughly normal.\nRelatively no multicollinearity based on the GVIFs.\n\nOur next logical step was to experiment with stabilizing the variance through transformations of our outcome.\nWe fitted various transformations, included four common ones (boxcox - lambda = 0.6, square root, logarithm, reciprocal), and compared their residual standard errors for further analysis.\n\n\n\nResidual Standard Errors by Transformation\n\n\nTransformation\nResidual SE\n\n\n\n\nsqrt(Income)\n59\n\n\nlog(Income + 1)\n1\n\n\n1 / Income\n0\n\n\nBC(λ=0.6)\n332\n\n\n\n\n\n\n\n\nFor the sqrt() transformation, the residual SE was 59 square root dollar units, showing some variance stabilization but not intuitive on this scale.\nFor the log() transformation, the residual SE was 1 log dollar unit, translating to multiplicative errors of ~2.7x in our residual calculations.\nFor the 1/Income reciprocal transformation, the residual SE was 0. The transformation collapsed all the variation towards 0 but over compressed to the point of breaking interpretability of the coefficients.\nFor the boxcox transformation, the residual SE was 332 (on lambda value of 0.6), comparable to the sqrt() transform (lambda value 0.5) but non-intuitive on a different scale.\n\nFurther analysis into the residual vs. fitted plots for all the transformations revealed that none of these transformations corrected for heteroscedasticity, with all showing funneling (and therefore we have omitted the plots for brevity).\nInstead of using these transformations, we decided to keep Income on its original scale and correct for valid inference by using robust standard errors through coeftest() for unbiased point estimates and accurate standard errors, t-stat values, and p-values under heteroscedasticity\n\n\n\nHC1‐Robust coeftest t‐tests for OLS Coefficients\n\n\nterm\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\nSignif\n\n\n\n\n(Intercept)\n19662.672\n5286.113\n3.720\n2.00e-04\n***\n\n\nRaceHispanic\n2807.627\n654.545\n4.289\n1.81e-05\n***\n\n\nRaceNon-Black/Non-Hispanic\n2993.485\n501.590\n5.968\n2.47e-09\n***\n\n\nSexMale\n14242.081\n410.099\n34.728\n&lt; 2e-16\n***\n\n\nMarital_StatusDivorced\n4408.501\n713.909\n6.175\n6.83e-10\n***\n\n\nMarital_StatusMarried\n6627.434\n611.675\n10.835\n&lt; 2e-16\n***\n\n\nMarital_StatusSeparated\n-1682.529\n1099.353\n-1.530\n1.26e-01\n\n\n\nMarital_StatusWidowed\n-677.400\n1486.064\n-0.456\n6.49e-01\n\n\n\nAge_2010\n-4.619\n92.115\n-0.050\n9.60e-01\n\n\n\nRegionNortheast\n4010.788\n702.725\n5.707\n1.17e-08\n***\n\n\nRegionSouth\n445.695\n516.859\n0.862\n3.89e-01\n\n\n\nRegionWest\n2824.512\n670.018\n4.216\n2.51e-05\n***\n\n\nedu_binHigh School\n10851.067\n1120.241\n9.686\n&lt; 2e-16\n***\n\n\nedu_binSome College\n18365.587\n1159.706\n15.836\n&lt; 2e-16\n***\n\n\nedu_binCollege+\n29432.746\n1190.501\n24.723\n&lt; 2e-16\n***\n\n\navg_LT_rate\n-2523.204\n574.428\n-4.393\n1.13e-05\n***\n\n\n\n\n\n\n\nAfter applying robust inference with coeftest(), our standard errors, t-values, and p-values were adjusted. Coefficients for predictors remained the same. We clustered by Case_ID to check for between-person correlation, but this did not differ from the original HC1 coeftest() and did not alter conclusions, so we only reported the original results. While there were no changes to significant terms, the p-value for average long-term interest rates lowered further.\nWith valid inferences now, the last part we decided to test was the use of stepwise AIC/BIC regression for variable selection.\n\n\n\nVariables Chosen by Stepwise AIC vs. BIC\n\n\n\n\n\n\nCriterion\nFormula\n\n\n\n\nAIC\nIncome ~ Race + Sex + Marital_Status + Region + edu_bin + avg_LT_rate\n\n\nBIC\nIncome ~ edu_bin + Sex + Marital_Status + Region + Race + avg_LT_rate\n\n\n\n\n\nBoth AIC and BIC criterion based selections show the same models, omitting Age_2010, aligning with the *** significance markers in our coefficient tests. While specific levels in variables (RegionSouth, Marital_StatusSeparated, etc) are not considered significant, the overall variables they belong to are significant as other levels have an effect. These stepwise regression results need to be considered with caution as heteroscedasticity violates the penalties enacted by AIC and BIC. More principled selection methods can be explored, but these are out of our scope."
  },
  {
    "objectID": "analysis.html#conclusion",
    "href": "analysis.html#conclusion",
    "title": "Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nBased on our diagnostics and overall checks, the following is our OLS model on the untransformed Income scale:\n\\[\n\\mathrm{Income}_i \\;=\\; \\beta_0\n  + \\beta_{\\text{Race}}\n  + \\beta_{\\text{Sex}}\n  + \\beta_{\\text{Marital}}\n  + \\beta_{\\text{Region}}\n  + \\beta_{\\text{Edu}}\n  + \\beta_{\\text{LT Rate}}\n  + \\varepsilon_i\n\\]\nOur pooled-OLS model (pooling 2006, 2008, and 2010 while retaining only long-term average interest rates) with HC1-Robust SEs (checked for person-clustered SEs) provided us with the following key takeaways:\nEducation and Gender remain the most dominant effects on Income: - With all other predictors held constant, males earned on average ~$14,242 more per year than females regardless of the year.\n\nAveraging across 2006-2010, college graduates earned nearly ~$30000 more than those who did not attend high school (“Less than HS” is part of the base estimate as an indicator variable). The gap is of a similar magnitude in each year, which suggests that a college education was stable even during the 2008 downturn.\nEach successive education category translated to about $7000-$11000 in additional annual earnings, a pattern stable across 2006-2010.\n\nRace, Marriage, and Region have strong secondary effects: - Individuals identifying as Non-Black/Non-Hispanic and Hispanic earned nearly $2993 and $2807 more than those identifying as Black / African-American (baseline), potentially indicative of Black / African-American individuals being disproportionately affected with relation to earnings.\n\nMarried individuals earned $6627 more than those who never married (baseline). Divorced individuals earned $4408 more, while separated and widowed individuals earned $1682 and $677 less, respectively. Marriage might have had an economical advantage (potentially dual incomes, shared expenses, financial support from spouse).\nCompared to those living in the central United States (baseline), residents in the Northeast earned ~$4010 more, and residents living in the West earned ~2824 more. The South had a modest increase in earnings of ~$445. These differences may reflect economic opportunities, urbanization, and higher costs of living.\n\nMacroeconomic conditions have an impact:\nBy omitting factor levels for each year, the long-term interest rates are isolated and show the marginal cyclical effect of the rates, not conflated with other effects embedded in the model and intercept (ex: wage growth, recessionary gaps, stimulus programs).\n\nA one-percent point rise in the average long-term interest rate is associated with a $2523 reduction in income on average across the three years, even after pooling across the recessionary dip in 2008.\n\nThis reduction does not account for total shock in 2008, rather reflecting on the credit-cost sensitivity, explaining the average difference in income due to the change in borrowing costs with all other predictors fixed."
  },
  {
    "objectID": "data.html#wherehow-to-find-data",
    "href": "data.html#wherehow-to-find-data",
    "title": "Data",
    "section": "Where/How to find Data",
    "text": "Where/How to find Data\nThe original dataset is the NLS National Longitudinal Survey, which can be found here. This data has been collected by the U.S. Bureau of Labor Statistics from 1970 - 2020 as part of a longitudinal study to better understand trends of labor force participation, education, and employment experiences evolving over time (with considerations of the effects of certain miscellaneous factors like race, gender, and socioeconomic status). This information will help paint a picture of the impacts the 2008 recession had on different racial and wealth groups in the United States of America.\nTo further explore recession disparities, the NLS dataset was to be combined with the interest rates archive of the Department of Treasury, which can be found here. The data is presented daily, can join average interest rate each year with yearly available data in our existing NLSY data, mainly income. This will highlight any relationships between annual interest rates and income stratified by race. All-in-all, analyzing this datset, in combination with the NLS dataset, will provide a clear picture of macroeconomic trends, and how they effect different race groups in America.\n\nDataSet #1 - NLS National Longitudinal Survey\n\nAll the R scripts we used to load and clean this dataset can be found here. Our cleaned dataset can be found here.\n\n\n\n\nNLS Variable Descriptions\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nCase_ID\nUnique identifier for each case or respondent\n\n\nSample_ID\nID representing the respondent’s sample group\n\n\nIncome_2000\nReported personal income in the year 2000 (in USD)\n\n\nIncome_2002\nReported personal income in the year 2002 (in USD)\n\n\nIncome_2004\nReported personal income in the year 2004 (in USD)\n\n\nIncome_2006\nReported personal income in the year 2006 (in USD)\n\n\nIncome_2008\nReported personal income in the year 2008 (in USD)\n\n\nIncome_2010\nReported personal income in the year 2010 (in USD)\n\n\nAge_2010\nRespondent’s age as of the year 2010\n\n\nRace\nSelf-reported race/ethnicity of the respondent\n\n\nSex\nSex of the respondent (e.g., Male/Female)\n\n\nHighest_Grade_Completed\nHighest level of education or grade completed by 2010\n\n\nMarital_Status\nMarital status of the respondent in 2010\n\n\nRegion\nGeographical region of residence in 2010"
  },
  {
    "objectID": "data.html#data-cleaning---nls-national-longitudinal-survey",
    "href": "data.html#data-cleaning---nls-national-longitudinal-survey",
    "title": "Data",
    "section": "Data Cleaning - NLS National Longitudinal Survey",
    "text": "Data Cleaning - NLS National Longitudinal Survey\nOur data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove “non-interview” data points that we can’t compare across years, and decode integer values representing race, grade, etc. For this process, we primarily depended on the tidyverse library and other methods discussed in lecture.\nnls_data &lt;- nls_data %&gt;%\n  mutate(\n    R6909701 = na_if(R6909701, -1),\n    R6909701 = ifelse(R6909701 &lt; 0, NA, R6909701),\n    R7607800 = na_if(R7607800, -1),\n    R7607800 = ifelse(R7607800 &lt; 0, NA, R7607800),\n    R8316300 = na_if(R8316300, -1),\n    R8316300 = ifelse(R8316300 &lt; 0, NA, R8316300),\n    T0912400 = na_if(T0912400, -1),\n    T0912400 = ifelse(T0912400 &lt; 0, NA, T0912400),\n    T2076700 = na_if(T2076700, -1),\n    T2076700 = ifelse(T2076700 &lt; 0, NA, T2076700),\n    T3045300 = na_if(T3045300, -1),\n    T3045300 = ifelse(T3045300 &lt; 0, NA, T3045300)\n  ) %&gt;%\nOur next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values. In doing so, we discovered other null values that required further debugging and cleaning.\n  # Remove rows where any variable is -5\n  filter(if_all(everything(), ~ . != -5))\nTo begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes.\n# 3. Define Grade Labels\ngrade_labels &lt;- c(\n  \"1st Grade\", \"2nd Grade\", \"3rd Grade\", \"4th Grade\", \"5th Grade\", \n  \"6th Grade\", \"7th Grade\", \"8th Grade\", \"9th Grade\", \"10th Grade\", \n  \"11th Grade\", \"12th Grade\", \"1st Year College\", \"2nd Year College\", \n  \"3rd Year College\", \"4th Year College\", \"5th Year College\", \"6th Year College\", \n  \"7th Year College\", \"8th Year College or More\", \"UNGRADED\"\n)\nAnother challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = “black”, 2 = “white”, etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns.\n# 4. Create Cleaned Dataset\nnls_data_clean &lt;- nls_data %&gt;%\n  mutate(\n    Race = case_when(\n      R0214700 == 1 ~ \"Hispanic\",\n      R0214700 == 2 ~ \"Black\",\n      R0214700 == 3 ~ \"Non-Black/Non-Hispanic\",\n      TRUE ~ NA_character_\n    ),\n    Sex = case_when(\n      R0214800 == 1 ~ \"Male\",\n      R0214800 == 2 ~ \"Female\",\n      TRUE ~ NA_character_\n    ),\n      Highest_Grade_Completed = case_when(\n        T2272800 == 1 ~ \"1ST GRADE\", \n        T2272800 == 2 ~ \"2ND GRADE\", \n        T2272800 == 3 ~ \"3RD GRADE\",\n        T2272800 == 4 ~ \"4TH GRADE\", \n        T2272800 == 5 ~ \"5TH GRADE\", \n        T2272800 == 6 ~ \"6TH GRADE\", \n        T2272800 == 7 ~ \"7TH GRADE\", \n        T2272800 == 8 ~ \"8TH GRADE\", \n        T2272800 == 9 ~ \"9TH GRADE\", \n        T2272800 == 10 ~ \"10TH GRADE\", \n        T2272800 == 11 ~ \"11TH GRADE\", \n        T2272800 == 12 ~ \"12TH GRADE\", \n        T2272800 == 13 ~ \"1ST YEAR COLLEGE\", \n        T2272800 == 14 ~ \"2ND YEAR COLLEGE\", \n        T2272800 == 15 ~ \"3RD YEAR COLLEGE\", \n        T2272800 == 16 ~ \"4TH YEAR COLLEGE\", \n        T2272800 == 17 ~ \"5TH YEAR COLLEGE\", \n        T2272800 == 18 ~ \"6TH YEAR COLLEGE\", \n        T2272800 == 19 ~ \"7TH YEAR COLLEGE\", \n        T2272800 == 20 ~ \"8TH YEAR COLLEGE OR MORE\", \n        T2272800 == 95 ~ \"UNGRADED\",\n        TRUE ~ NA_character_\n      ),\n  Marital_Status = case_when(\n      T3108400 == 0 ~ \"Never Married\",\n      T3108400 == 1 ~ \"Married\",\n      T3108400 == 2 ~ \"Separated\",\n      T3108400 == 3 ~ \"Divorced\",\n      T3108400 == 6 ~ \"Widowed\",\n      TRUE ~ NA_character_\n    ),\n  Region = case_when(\n    T3108200 == 1 ~ \"Northeast\",\n    T3108200 == 2 ~ \"North Central\",\n    T3108200 == 3 ~ \"South\",\n    T3108200 == 4 ~ \"West\",\n    TRUE ~ NA_character_\n  )\n  ) %&gt;%\n  # Convert categorical variables to factors\n  mutate(across(c(Race, Sex, Highest_Grade_Completed, Marital_Status, Region), as.factor)) %&gt;%\n  # Remove original numeric columns\n  select(-c(R0214700, R0214800, T2272800, T3108400, T3108200)) %&gt;%\n  # Rename columns\n  # Rename columns\n  rename(\n    Case_ID = R0000100,\n    Sample_ID = R0173600,\n    Age_2010 = T3108700,\n    Income_2000 = R6909701,\n    Income_2002 = R7607800,\n    Income_2004 = R8316300,\n    Income_2006 = T0912400,\n    Income_2008 = T2076700,\n    Income_2010 = T3045300\n  )"
  },
  {
    "objectID": "data.html#summary-statistics---nls-national-longitudinal-survey",
    "href": "data.html#summary-statistics---nls-national-longitudinal-survey",
    "title": "Data",
    "section": "Summary Statistics - NLS National Longitudinal Survey",
    "text": "Summary Statistics - NLS National Longitudinal Survey\nHere are some important variables and their distributions found in the NLS data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataSet #2 - Interest Rates by Year\n\nThis cleaned dataset can be found here.\n\n\n\n\nInterest Rate Variable Descriptions\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nYear\nYear the data was collected\n\n\nmean_LT\nMean of long-term interest rates (&gt;10 years)\n\n\nmin_LT\nMinimum of long-term interest rates (&gt;10 years)\n\n\nmax_LT\nMaximum of long-term interest rates (&gt;10 years)\n\n\nsd_LT\nStandard deviation of long-term interest rates (&gt;10 years)\n\n\nmedian_LT\nMedian of long-term interest rates (&gt;10 years)\n\n\nmode_LT\nMode of long-term interest rates (&gt;10 years)\n\n\nmean_TREASURY\nMean of mortgage interest rates\n\n\nmin_TREASURY\nMinimum of mortgage interest rates\n\n\nmax_TREASURY\nMaximum of mortgage interest rates\n\n\nsd_TREASURY\nStandard deviation of mortgage interest rates\n\n\nmedian_TREASURY\nMedian of mortgage interest rates\n\n\nmode_TREASURY\nMode of mortgage interest rates"
  },
  {
    "objectID": "data.html#data-cleaning---interest-rates-by-year",
    "href": "data.html#data-cleaning---interest-rates-by-year",
    "title": "Data",
    "section": "Data Cleaning - Interest Rates by Year",
    "text": "Data Cleaning - Interest Rates by Year\nWe began by importing the dataset using the read_csv() function from the readr package in the tidyverse. Then, we first added additional income years to cover 2000-2010 as opposed to just 2010. Afterwards, we joined the interest rate average of each year with the corresponding two years - so 2001 and 2002 went into 2002, 2003 and 2004 went into 2004, and so on. By doing this, we were able to join the interest rate data into our survey data in an efficient and complementary way.\n# Load in another library to clean second dataset\nlibrary(lubridate)\n\n# Now we clean the second Dataset which we loaded in \nrates &lt;- read_csv(\"./dataset/long-term-rates-2000-2010.csv\")\nrates2 &lt;- read_csv(\"./dataset/long-term-rates-2011-2020.csv\")\n\n#combine rates and rates2\ncombined_rates &lt;- bind_rows(rates1, rates2)\nAdditionally, defining a custom get_mode() function, made it easy to handle mode calculation, since R does not provide a built-in mode() for numeric data in a tidy pipeline.\n# Define a mode function (returns the first mode if multiple)\nget_mode &lt;- function(x) {\n  ux &lt;- unique(na.omit(x))\n  ux[which.max(tabulate(match(x, ux)))]\n}\nThe Date column, originally in character format, was converted to proper Date objects using lubridate::mdy(). We then extracted the year from each date to enable grouping by year. To understand yearly trends in interest rates, we grouped the data by Year and computed a comprehensive set of summary statistics for each of the two rate columns including mean, minimum, maximum, standard deviation, median, and mode.\n#mutate data to have proper year date format (wil be easier to compute stat summaries with)\ninterest_rates_cleaned &lt;- combined_rates %&gt;%\n  mutate(Date = mdy(Date),\n         Year = year(Date)) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    mean_LT = mean(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    min_LT = min(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    max_LT = max(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    sd_LT = sd(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    median_LT = median(`LT COMPOSITE (&gt;10 Yrs)`, na.rm = TRUE),\n    mode_LT = get_mode(`LT COMPOSITE (&gt;10 Yrs)`),\n    \n    mean_TREASURY = mean(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    min_TREASURY = min(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    max_TREASURY = max(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    sd_TREASURY = sd(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    median_TREASURY = median(`TREASURY 20-Yr CMT`, na.rm = TRUE),\n    mode_TREASURY = get_mode(`TREASURY 20-Yr CMT`)\n  ) %&gt;%\n  ungroup()\nFinally, the dataset is written on to a CSV file for later use.\nwrite_csv(interest_rates_cleaned, \"interest_rates_cleaned.csv\")"
  },
  {
    "objectID": "data.html#summary-statistics---interest-rates",
    "href": "data.html#summary-statistics---interest-rates",
    "title": "Data",
    "section": "Summary Statistics - Interest Rates",
    "text": "Summary Statistics - Interest Rates\nHere are some important variables and their distributions found in the Interest Rates data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Combination\n\nThis cleaned dataset can be found here.\n\nWe begin by loading our cleaned second dataset. Since the interest rates were available annually but the NLS income data was collected biennially, we created a mapping table (rate_to_income) that associates each available rate year with the closest corresponding income year.\n\nlibrary(tidyverse)\n\n# Load the cleaned interest rate data\ninterest_rates &lt;- read_csv(here(\"dataset\", \"interest_rates_cleaned.csv\"))\n\n# Map rate years to income years\nrate_to_income &lt;- tribble(\n  ~Year, ~Income_Year,\n  2000, 2000,\n  2001, 2002,\n  2002, 2002,\n  2003, 2004,\n  2004, 2004,\n  2005, 2006,\n  2006, 2006,\n  2007, 2008,\n  2008, 2008,\n  2009, 2010,\n  2010, 2010\n)\nUsing the mapping, we joined the interest rates to their matched income years. Then, we grouped the data by Income_Year and computed the mean of the long-term interest rates (avg_LT_rate) for each income year.\n# Join and average interest rates by income year\nmapped_rates &lt;- rate_to_income %&gt;%\n  left_join(interest_rates, by = \"Year\") %&gt;%\n  group_by(Income_Year) %&gt;%\n  summarise(avg_LT_rate = mean(mean_LT, na.rm = TRUE)) %&gt;%\n  ungroup()\nThe NLS dataset was originally wide, with separate columns for each year’s income. We reshaped it into a long format where each row represents an individual-year combination, keeping demographic variables unchanged. The income year was extracted from the column names and converted into an integer for proper joining.\n# Step 1: Pivot NLS data to long format while keeping demographic columns\nnls_income_long &lt;- nls_data_clean %&gt;%\n  pivot_longer(\n    cols = starts_with(\"Income_\"),\n    names_to = \"Income_Year\",\n    names_prefix = \"Income_\",\n    values_to = \"Income\"\n  ) %&gt;%\n  mutate(Income_Year = as.integer(Income_Year))\nWe then merged the long-format NLS income data with the mapped interest rates using Income_Year as the key. This appended the relevant avg_LT_rate to each income record. Finally, we saved the fully combined dataset (nls_with_rates_full.csv) for future modeling and analysis.\n\n# Step 2: Join average interest rates by income year\nnls_with_rates_full &lt;- nls_income_long %&gt;%\n  left_join(mapped_rates, by = \"Income_Year\")\n\nwrite_csv(nls_with_rates_full, here(\"dataset\", \"nls_with_rates_full.csv\"))"
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html",
    "title": "Blog Post #1",
    "section": "",
    "text": "This is DataDetectives’ first blog post, where we describe potential datasets to use for our final project.\n\n\n\n130 rows x 20 columns | U.S. Census Bureau’s 2023 Survey of Income and Program Participation (SIPP) | DataSet\n\nThe data is well organized but it is not in tsv/csv format so it will have to be converted before being loaded and cleaned. Questions we hope to address: What is the distribution of household wealth across different demographic and economic groups?, How do asset ownership and debt-holding rates vary among households?, What insights can be drawn about economic well-being based on household wealth variations? Some foreseen challenges: It may be difficult to determine whether factors like education and income cause differences in wealth or are merely associated with them, Variability in how households report asset values and debt could introduce inconsistencies in wealth calculations, and The dataset reflects household wealth at the end of 2022, a period influenced by inflation, market fluctuations, and post-pandemic recovery, which could affect interpretability.\n\n\n\n\n253 columns x 16384 rows | The U.S. Geological Survey | DataSet\n\nWe hope to use groundwater consumption and withdrawal data from the U.S. geological survey for our final project. This data outlines monthly public supply, irrigation, and thermoelectric water use from 2009 - 2020. The dataset, which comes from a federal source, already comes in a CSV file, so it is easy to clean, read, and manage. With this information, we hope to address the question: Do different communities’ drinking water lead to health disparities? However, this leads to the challenges with this particular dataset. On its own, the dataset doesn’t provide information that can answer a question related to racial disparities. In order to answer our questions we would need to collect US Census Data, which will give us demographics (age/race) for the particular year we decide to evaluate. While census data is robust, we will be able to filter the information to better support this initial dataset.\n\n\n\n\nMany columns and rows | Bureau of Labor Service | DataSet\n\nThe NLSY is a cohort of surveys administered to the same group of several thousand respondents over decades. The one we want to look at starts from 1979 and goes on until 2022, with the 1979 respondents being aged 14 to 18. Each survey, they are asked various questions from socioeconomic status to health and family size. Due to the breadth of the data available, there is an online data selection tool that allows for specific columns to be exported as CSV files for convenient use, and there are thousands of variables to choose from. Some questions we could answer are: what factors contribute most significantly to upward economic mobility over time? How do employment trends and income differ by gender, race, and education level? Are there racial disparities on long-term income and wealth? Visualization might be a bit difficult directly and it would likely require us to look at proportions instead. However, it’s nice that there is so much available content we could look at."
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-1---wealth-of-households-2022",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-1---wealth-of-households-2022",
    "title": "Blog Post #1",
    "section": "",
    "text": "130 rows x 20 columns | U.S. Census Bureau’s 2023 Survey of Income and Program Participation (SIPP) | DataSet\n\nThe data is well organized but it is not in tsv/csv format so it will have to be converted before being loaded and cleaned. Questions we hope to address: What is the distribution of household wealth across different demographic and economic groups?, How do asset ownership and debt-holding rates vary among households?, What insights can be drawn about economic well-being based on household wealth variations? Some foreseen challenges: It may be difficult to determine whether factors like education and income cause differences in wealth or are merely associated with them, Variability in how households report asset values and debt could introduce inconsistencies in wealth calculations, and The dataset reflects household wealth at the end of 2022, a period influenced by inflation, market fluctuations, and post-pandemic recovery, which could affect interpretability."
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-2---public-water-use",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-2---public-water-use",
    "title": "Blog Post #1",
    "section": "",
    "text": "253 columns x 16384 rows | The U.S. Geological Survey | DataSet\n\nWe hope to use groundwater consumption and withdrawal data from the U.S. geological survey for our final project. This data outlines monthly public supply, irrigation, and thermoelectric water use from 2009 - 2020. The dataset, which comes from a federal source, already comes in a CSV file, so it is easy to clean, read, and manage. With this information, we hope to address the question: Do different communities’ drinking water lead to health disparities? However, this leads to the challenges with this particular dataset. On its own, the dataset doesn’t provide information that can answer a question related to racial disparities. In order to answer our questions we would need to collect US Census Data, which will give us demographics (age/race) for the particular year we decide to evaluate. While census data is robust, we will be able to filter the information to better support this initial dataset."
  },
  {
    "objectID": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-3---national-longitudinal-survey-of-youth",
    "href": "posts/2025-02-26-blog-post-1/blog-post-1.html#dataset-3---national-longitudinal-survey-of-youth",
    "title": "Blog Post #1",
    "section": "",
    "text": "Many columns and rows | Bureau of Labor Service | DataSet\n\nThe NLSY is a cohort of surveys administered to the same group of several thousand respondents over decades. The one we want to look at starts from 1979 and goes on until 2022, with the 1979 respondents being aged 14 to 18. Each survey, they are asked various questions from socioeconomic status to health and family size. Due to the breadth of the data available, there is an online data selection tool that allows for specific columns to be exported as CSV files for convenient use, and there are thousands of variables to choose from. Some questions we could answer are: what factors contribute most significantly to upward economic mobility over time? How do employment trends and income differ by gender, race, and education level? Are there racial disparities on long-term income and wealth? Visualization might be a bit difficult directly and it would likely require us to look at proportions instead. However, it’s nice that there is so much available content we could look at."
  },
  {
    "objectID": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "href": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Here are some basic figures we created after cleaning, these are meant to show a quick breakdown of some of the variables we cleaned.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\n#count of races that participated \nggplot(nls_data_clean, aes(x = Race, fill = Sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Participant Count by Race and Sex\", x = \"Race\", y = \"Count\")\n\n\n\n\n\n\n\n#count of different marital statuses found in survey\nggplot(nls_data_clean, aes(x = Marital_Status)) +\n  geom_bar(fill = \"orchid\") +\n  labs(title = \"Marital Status Distribution\", x = \"Marital Status\", y = \"Count\")\n\n\n\n\n\n\n\n\nOur data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove “non-interview” data points that we can’t compare across years, and decode integer values representing race, grade, etc. For this process, we primarily depended on the tidyverse library and other methods discussed in lecture.\nTo begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes. Our next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values.\nAnother challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = “black”, 2 = “white”, etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns.\nNow that our initial dataset is clean we plan to create more visuals to analyze this set, determine our next dataset to combine with this one, choose which additional data from this source to include (and clean of course)."
  },
  {
    "objectID": "posts/2025-03-24-blog-post-3/blog-post-3.html#data-cleaning",
    "href": "posts/2025-03-24-blog-post-3/blog-post-3.html#data-cleaning",
    "title": "Blog Post 3",
    "section": "",
    "text": "Here are some basic figures we created after cleaning, these are meant to show a quick breakdown of some of the variables we cleaned.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnls_data_clean &lt;- read_rds(\"dataset/nls_clean.rds\")\n\n#count of races that participated \nggplot(nls_data_clean, aes(x = Race, fill = Sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Participant Count by Race and Sex\", x = \"Race\", y = \"Count\")\n\n\n\n\n\n\n\n#count of different marital statuses found in survey\nggplot(nls_data_clean, aes(x = Marital_Status)) +\n  geom_bar(fill = \"orchid\") +\n  labs(title = \"Marital Status Distribution\", x = \"Marital Status\", y = \"Count\")\n\n\n\n\n\n\n\n\nOur data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove “non-interview” data points that we can’t compare across years, and decode integer values representing race, grade, etc. For this process, we primarily depended on the tidyverse library and other methods discussed in lecture.\nTo begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes. Our next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values.\nAnother challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = “black”, 2 = “white”, etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns.\nNow that our initial dataset is clean we plan to create more visuals to analyze this set, determine our next dataset to combine with this one, choose which additional data from this source to include (and clean of course)."
  },
  {
    "objectID": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "href": "posts/2025-04-07-blog-post-5/blog-post-5.html",
    "title": "Blog Post #5",
    "section": "",
    "text": "For this blog post, we sought out to add a new dataset to our project. After discussing, we decided on the interest rates archive of the Department of Treasury (Link: https://home.treasury.gov/interest-rates-data-csv-archive). The data is presented daily, and to join with our existing data nicely, we thought of looking at the average interest rate each year and joining it with yearly available data in our existing NLSY data, mainly income. We plan on seeing if there’s any relationship between annual interest rates and income stratified by race. Up until this point, we had only looked at income data from 2010, so we will need to download income data for years 2000-2020 and expand the NLSY dataset. This should not be an issue, but we will have to slightly modify the cleaning code to account for the new years. Then, we downloaded Daily Treasury Long-Term Rates from the website. Since these are relatively stable as long term rates, aggregating over a year and averaging should give us a good idea of the interest rate for that year. Overall, this should be a good way to see if broader macroeconomic conditions impact the financial wellbeing of people different backgrounds in different ways."
  },
  {
    "objectID": "posts/2025-04-21-blog-post-7/blog-post-7.html",
    "href": "posts/2025-04-21-blog-post-7/blog-post-7.html",
    "title": "Blog Post #7",
    "section": "",
    "text": "Blog post 7\n\n\nOur Interactive - Design Goals\nWe plan to use an interactive to analyze the impact of the 2008 recession on different groups of people. We plan to allow users to select one or more features like race, marital status, income, etc. to compare.\nOne idea– a user can start by viewing a line chart showing income trends across education levels from 2000 to 2010, then zoom into subgroups defined by race, gender, or marital status. We can include tooltips, highlighted callouts, brief annotations, or even letting users “find themselves” with their own information.\nAnother idea– we can create two maps graphics with identical statistics that the user changes, but one from 2008 and another from 2010. This way, the user can see the impact of the recession on different racial groups in different US regions.\n\n\nOur Interactive - Progress so Far\nSo far, most of our intial data cleaning and combining is finally complete. From last week, we combined our two datasets to create a more holistic view of the macroeconomic trends in America during the 2008 recession.\nNow, we plan on using these datasets to create the interactive feature using shiny or quarto::interactive().\n\n\nOur Interactive - Next Steps\nStep 1: Finalize the UI design and incorporate dropdowns for user selection. Step 2: Implement components to adjust plots/statistics based on user input with R script. This step will liekly take the longest and involve the most debugging. Step 3: Write captions and annotations to help guide interpretation. Step 4: Test the app with users to ensure it’s intuitive and engaging."
  }
]