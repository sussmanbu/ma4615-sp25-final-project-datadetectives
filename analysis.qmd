---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: false
---

```{r setup, include=FALSE}
library(here)
library(tidyverse)  # or specific: readr, dplyr, etc.
```

![](https://upload.wikimedia.org/wikipedia/commons/7/77/Pebbleswithquarzite.jpg)

This comes from the file `analysis.qmd`.

We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You'll also reflect on next steps and further analysis.

The audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.

While the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.

The style of this paper should aim to be that of an academic paper. I don't expect this to be of publication quality but you should keep that aim in mind. Avoid using "we" too frequently, for example "We also found that ...". Describe your methodology and your findings but don't describe your whole process.

### Example of loading data

The code below shows an example of loading the loan refusal data set (which you should delete at some point).

```{r}
library(tidyverse)
print(getwd())
```

## Note on Attribution

In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation.

If you are directly quoting from a source, please make that clear. You can show long quotes using `>` like this

```         
> To be or not to be.
```

> To be or not to be.

------------------------------------------------------------------------

## Rubric: On this page

You will

-   Introduce what motivates your Data Analysis (DA)
    -   Which variables and relationships are you most interested in?
    -   What questions are you interested in answering?
    -   Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.
-   Modeling and Inference
    -   The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.
    -   Explain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)
    -   Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.
-   Explain the flaws and limitations of your analysis
    -   Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?
-   Clarity Figures
    -   Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?
    -   Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)
    -   Default `lm` output and plots are typically not acceptable.
-   Clarity of Explanations
    -   How well do you explain each figure/result?
    -   Do you provide interpretations that suggest further analysis or explanations for observed phenomenon?
-   Organization and cleanliness.
    -   Make sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.
    -   This page should be self-contained, i.e. provide a description of the relevant data.

## Introduction

### Objective and Research Questions

The goal of our analysis is to examine how macroeconomic trends in the aftermath of the 2008 recession, such as shifting interest rates, affected different racial and socioeconomic groups in the United States. We aim to answer some of the following research questions:

-   What factors contribute to upward economic mobility?
-   How do income and employment differ across gender, race, and education levels?
-   Are there persistent racial disparities in long-term wealth accumulation?

### Motivating Figures and Tables

The following graphs explore some of the relationships between variables in our data to motivate this goal.

```{r chandini graphs, echo= FALSE}
library(scales)
nls_data_clean <- read_rds("dataset/nls_clean.rds")

#define correct order of education levels(needed for next query)
education_levels <- c(
  "1ST GRADE", "2ND GRADE", "3RD GRADE", "4TH GRADE", "5TH GRADE", "6TH GRADE", 
  "7TH GRADE", "8TH GRADE", "9TH GRADE", "10TH GRADE", "11TH GRADE", "12TH GRADE", 
  "1ST YEAR COLLEGE", "2ND YEAR COLLEGE", "3RD YEAR COLLEGE", "4TH YEAR COLLEGE", 
  "5TH YEAR COLLEGE", "6TH YEAR COLLEGE", "7TH YEAR COLLEGE", "8TH YEAR COLLEGE OR MORE",
  "UNGRADED", NA
)

#mutate to order based on most education completed 
nls_data_clean <- nls_data_clean %>%
  mutate(Highest_Grade_Completed = factor(Highest_Grade_Completed, 
                                          levels = education_levels,
                                          ordered = TRUE))
#Calcualte average mean_income for each grade category
income_by_grade <- nls_data_clean %>%
  group_by(Highest_Grade_Completed) %>%
  summarise(mean_income = mean(Income_2010, na.rm = TRUE)) %>%
  filter(!is.na(Highest_Grade_Completed))

#income vs highest grade plotted with ggplot
ggplot(income_by_grade, aes(x = Highest_Grade_Completed, y = mean_income, group = 1)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(color = "darkred", size = 2) +
  labs(title = "Average Income by Highest Grade Completed",
       x = "Highest Grade Completed",
       y = "Average Income") +
  scale_y_continuous(labels = label_comma()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

```{r aretha graphs, echo= FALSE}
library(scales)
nls_data_clean <- read_rds("dataset/nls_clean.rds")

nls_data_clean <- nls_data_clean %>%
  filter(!is.na(Region))

# Regional Variation in Gender Income Gap
ggplot(nls_data_clean, aes(x = Region, y = Income_2010, fill = Sex)) +
  geom_boxplot(outlier.alpha = 0.5, outlier.size = 1) +
  labs(title = "Income Distribution by Region and Gender",
       x = "Region", 
       y = "Total Income (Previous Year)", 
       fill = "Gender") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(labels = scales::comma)
```

##### This box plot explores the depth of the relationship between income, gender and region to examine whether there are any disparities between genders.

```{r ran graphs, echo= FALSE}
library(scales)
nls_data_clean <- read_rds("dataset/nls_clean.rds")

nls_data_clean_filtered <- nls_data_clean %>%
  filter(!is.na(Income_2010), is.finite(Income_2010))

ggplot(nls_data_clean_filtered, aes(x = factor(Age_2010), y = Income_2010)) + geom_boxplot() + facet_wrap(~ Sex + Race) + scale_y_continuous(labels = dollar_format(prefix = "$", suffix = "K", scale = 1/1000)) + labs(x = "Age in 2010", y = "Total Income for Year", title = "Total Income For Year 2010 Filtered by Age, Race, and Sex") + theme_minimal()

```

### Now, we look at interest rates.

```{r datacombination, echo= FALSE}

nls_with_rates_full<-read_csv("./dataset/nls_with_rates_full.csv")

income_by_race <- nls_with_rates_full %>%
  group_by(Income_Year, Race) %>%
  summarise(
    avg_income = mean(Income, na.rm = TRUE),
    avg_interest = mean(avg_LT_rate, na.rm = TRUE)
  ) %>%
  ungroup()
library(ggplot2)

ggplot(income_by_race, aes(x = Income_Year, y = avg_income, color = avg_interest)) +
  geom_line(aes(group = Race), size = 1.2) +
  geom_point(aes(shape = Race), size = 2) +
  facet_wrap(~ Race) +
  scale_color_viridis_c(option = "magma") +
  labs(
    title = "Average Income Over Time by Race (Colored by Interest Rate)",
    x = "Year",
    y = "Average Income",
    color = "Interest Rate",
    shape = "Race"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

## Modeling and Inference

First tried with outliers and had bad figures, and we determined there were severe outliers impacting our model. 

```{r outliers, echo= FALSE}
library(tidyverse)

# Load the data
nls_with_rates <- read_csv("dataset/nls_with_rates_full.csv")


# Calculate IQR and bounds
Q1 <- quantile(nls_with_rates$Income, 0.25, na.rm = TRUE)
Q3 <- quantile(nls_with_rates$Income, 0.75, na.rm = TRUE)
IQR_value <- Q3 - Q1

lower_bound <- Q1 - .20 * IQR_value
upper_bound <- Q3 + 1.75 * IQR_value

# Plot histogram with cutoff lines
ggplot(nls_with_rates, aes(x = Income)) +
  geom_histogram(binwidth = 5000, color = "black", fill = "lightblue") +
  geom_vline(xintercept = lower_bound, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = upper_bound, color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = lower_bound, y = 3000, label = "Lower Bound", angle = 90, vjust = -0.5, color = "red") +
  annotate("text", x = upper_bound, y = 3000, label = "Upper Bound", angle = 90, vjust = -0.5, color = "red") +
  labs(title = "Histogram of Income with Outlier Cutoff Lines",
       subtitle = "Red dashed lines indicate outlier thresholds",
       x = "Income",
       y = "Count") +
  theme_minimal()

```
We therefore used R script like below to remove these and create a new dataset specifically for modeling. 

```r
library(tidyverse)

# Load  merged dataset
nls_with_rates <- read_csv("dataset/nls_with_rates_full.csv")

# Step 1: Calculate IQR boundaries
Q1 <- quantile(nls_with_rates$Income, 0.25, na.rm = TRUE)
Q3 <- quantile(nls_with_rates$Income, 0.75, na.rm = TRUE)
IQR_value <- Q3 - Q1

lower_bound <- Q1 - 0.24 * IQR_value #lots of 0 (n/a) data
upper_bound <- Q3 + 1.5 * IQR_value

# Step 2: Filter out the outliers
nls_no_outliers <- nls_with_rates %>%
  filter(Income >= lower_bound & Income <= upper_bound)

# Step 3: Save it
write_csv(nls_no_outliers, "dataset/nls_no_outliers.csv")
```

```{r statmod}

library(scales)
library(car)
library(sandwich)
 
nls_data_tbu <- read_csv("dataset/nls_no_outliers.csv")
 
nls_data_modeling <- nls_data_tbu %>%
   filter(Income_Year %in% c(2006, 2008, 2010))
 
model = lm(Income ~ Race + Sex + Marital_Status + Age_2010 + Region + Highest_Grade_Completed + avg_LT_rate, data = filter(nls_data_modeling, Income > 0))
 
# coeftest(model, vcov = vcovHC(model, type = "HC1"))
 
model_data = model.frame(model)
model_data$Predicted = predict(model)
summary(model)
vif(model, type = "predictor")
plot(model)

```

## Results, Limitations, and Conclusion
