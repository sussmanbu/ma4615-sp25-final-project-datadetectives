---
title: "Blog Post 3"
subtitle: "Data Loading/Cleaning"
description:  |
  This is Data Detectives' third blog post where we further investigate, load,
  and clean our initial dataset.
author: "Data Detectives"
date: "2025-03-24"
date-modified: "2025-03-24"
draft: FALSE
---
  
## Data Cleaning

TODO:  Use figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.

Our data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove "non-interview" data points that we can't compare across years, and decode integer values representing race, grade, etc. For this process, we primarily depended on the tidyverse library and other methods discussed in lecture. 

To begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes.  Our next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values. 

Another challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = "black", 2 = "white", etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns. 

Now that our initial dataset is clean we plan to create more visuals to analyze this set, determine our next dataset to combine with this one, choose which additional data from this source to include (and clean of course). 


