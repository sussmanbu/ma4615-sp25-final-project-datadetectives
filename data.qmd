---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
---

![](images/data-import-cheatsheet-thumbs.png)


This comes from the file `data.qmd`.

Your first steps in this project will be to find data to work on.

I recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.


Initially, you will study _one dataset_ but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable.
Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components.


## What makes a good data set?

* Data you are interested in and care about.
* Data where there are a lot of potential questions that you can explore.
* A data set that isn't completely cleaned already.
* Multiple sources for data that you can combine.
* Some type of time and/or location component.


## Where to keep data?

Below 50mb: In `dataset` folder

Above 50mb: In `dataset-ignore` folder which you will have to create manually. This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data

For small datasets (<50mb), you can use the `dataset` folder that is tracked by github. Stage and commit the files just like you would any other file.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`. This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits. Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [clean_data.R](/scripts/clean_data.R) file in the `scripts` folder is the file where you will import the raw data that you download, clean it, and write `.rds` file(s) (using `write_rds`) that you'll load in your analysis page. If desirable, you can have multiple scripts that produce different derived data sets, just make sure to link to them on this page.

You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\MA415\\Final_Project\`).
Instead, use the `here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.

### Clean data script

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which will usually be `.rds` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.
To link to this file, you can use `[cleaning script](/scripts/clean_data.R)` wich appears as [cleaning script](/scripts/clean_data.R). 

----

## Rubric: On this page

You will

### Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.
  * Use figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `clean_data.R` file.
  * Rename variables and recode factors to make data more clear.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
  * This page should be self-contained.
  

## Our DataSet 
* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.
  
## Data Cleaning
TODO:  Use figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.
TODO: link clean_data.R in placeholder

Our data set required significant cleaning. In order to work with our CSV file, we needed to decode the integers that served as placeholders for column names, remove "non-interview" data points that we can't compare across years, and decode integer values representing race, grade, etc. All the R scripts we used to load and clean the data can be found [here](https://www.bls.gov/nls/nlsy79.htm). For this process, we primarily depended on the tidyverse library and other methods discussed in lecture. 

To begin with, working with integer-coded columns would have been incredibly difficult. To combat this issue, we created began with gathering all the integer codes and their String equivalents. Next, we created R scripts to rename all our selected columns based on their integer codes.  

The script is in our clean_data.R as follows: 

```{r c1}
# NOTE: Salary and wages are already decoded, Age at Interview Data already decoded

library(tidyverse)
library(here)

# 1. Load Data with Explicit Column Types
nls_data <- read_csv(
  here("dataset", "nls_initial_data_updated.csv"),
  col_types = cols(
    R0000100 = col_integer(),   # Case ID
    R0173600 = col_integer(),   # Sample ID
    R0214700 = col_integer(),   # Race
    R0214800 = col_integer(),   # Sex
    T2272800 = col_integer(),   # Highest Grade Completed
    T3045300 = col_double(),    # Total Income
    T3108400 = col_integer(),   # Marital Status
    T3108700 = col_integer()    # Age
  )
) %>%

```

Our next task was removing non-interview values. These rows, indicated by -5 values, imply that the interviewee either quit or was removed from the data sampling. Therefore, they may be included in earlier years and not in late years. To improve our modeling, we must remove these values. 

In doing so, we discovered other null values that required further debugging and cleaning. The R script to do both these tasks is as follows:
```{r c2}
  # Remove rows where any variable is -5
  filter(if_all(everything(), ~ . != -5))

# Debugging Step: Check if `nls_data` is created
print("nls_data successfully loaded and filtered.")
print(dim(nls_data))

# 2. Handle Missing Data and Negative Income
nls_data <- nls_data %>%
  mutate(
    T3045300 = na_if(T3045300, -1),  # Mark -1 as NA
    T3045300 = ifelse(T3045300 < 0, NA, T3045300)  # Negative income to NA
  )

# Debugging Step: Check after missing value handling
print("Handled missing data and negative income.")

```

Another challenge was modifying actual data points in every row. For example, rather than listing a respondents race, the information is coded in integers (i.e. 1 = "black", 2 = "white", etc.) This would be incredibly difficult to keep track of in the long term scope of our project, therefore, we decided to modify these values. 

Columns race, sex, highest grade completed, and marital status were all encoded using this style. Consequently, we used similar R scripts to decode all of these columns. The R script to do so is as follows: 
```{r c3}
# 3. Define Grade Labels
grade_labels <- c(
  "1st Grade", "2nd Grade", "3rd Grade", "4th Grade", "5th Grade", 
  "6th Grade", "7th Grade", "8th Grade", "9th Grade", "10th Grade", 
  "11th Grade", "12th Grade", "1st Year College", "2nd Year College", 
  "3rd Year College", "4th Year College", "5th Year College", "6th Year College", 
  "7th Year College", "8th Year College or More", "UNGRADED"
)

# 4. Create Cleaned Dataset
nls_data_clean <- nls_data %>%
  mutate(
    Race = case_when(
      R0214700 == 1 ~ "Hispanic",
      R0214700 == 2 ~ "Black",
      R0214700 == 3 ~ "Non-Black/Non-Hispanic",
      TRUE ~ NA_character_
    ),
    Sex = case_when(
      R0214800 == 1 ~ "Male",
      R0214800 == 2 ~ "Female",
      TRUE ~ NA_character_
    ),
      Highest_Grade_Completed = case_when(
        T2272800 == 1 ~ "1ST GRADE", 
        T2272800 == 2 ~ "2ND GRADE", 
        T2272800 == 3 ~ "3RD GRADE",
        T2272800 == 4 ~ "4TH GRADE", 
        T2272800 == 5 ~ "5TH GRADE", 
        T2272800 == 6 ~ "6TH GRADE", 
        T2272800 == 7 ~ "7TH GRADE", 
        T2272800 == 8 ~ "8TH GRADE", 
        T2272800 == 9 ~ "9TH GRADE", 
        T2272800 == 10 ~ "10TH GRADE", 
        T2272800 == 11 ~ "11TH GRADE", 
        T2272800 == 12 ~ "12TH GRADE", 
        T2272800 == 13 ~ "1ST YEAR COLLEGE", 
        T2272800 == 14 ~ "2ND YEAR COLLEGE", 
        T2272800 == 15 ~ "3RD YEAR COLLEGE", 
        T2272800 == 16 ~ "4TH YEAR COLLEGE", 
        T2272800 == 17 ~ "5TH YEAR COLLEGE", 
        T2272800 == 18 ~ "6TH YEAR COLLEGE", 
        T2272800 == 19 ~ "7TH YEAR COLLEGE", 
        T2272800 == 20 ~ "8TH YEAR COLLEGE OR MORE", 
        T2272800 == 95 ~ "UNGRADED",
        TRUE ~ NA_character_
      ),
  Marital_Status = case_when(
      T3108400 == 0 ~ "Never Married",
      T3108400 == 1 ~ "Married",
      T3108400 == 2 ~ "Separated",
      T3108400 == 3 ~ "Divorced",
      T3108400 == 6 ~ "Widowed",
      TRUE ~ NA_character_
    )
  ) %>%
  # Convert categorical variables to factors
  mutate(across(c(Race, Sex, Highest_Grade_Completed, Marital_Status), as.factor)) %>%
  # Remove original numeric columns
  select(-c(R0214700, R0214800, T2272800, T3108400)) %>%
  # Rename columns
  rename(
    Case_ID = R0000100,
    Sample_ID = R0173600,
    Total_Income_Prev_Year = T3045300,
    Age_2010 = T3108700
  )

```

  
## Data Combination

 * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.
  * This page should be self-contained.
